"""
èŠå¤©API - å¯¹å¤–æä¾›æµå¼èŠå¤©æ¥å£
"""
import asyncio
import json
import logging
from typing import Dict, Any, Optional, AsyncGenerator, List
from datetime import datetime
from fastapi import APIRouter, HTTPException, Request
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field

from ..services.ai_server import AiServer
from ..services.ai_request_handler import AiModelConfig, Message, CallbackType
from ..services.mcp_tool_execute import create_example_mcp_executor, McpToolExecute
from ..services.stream_manager import stream_manager
from ..models import db
from ..models.message import MessageCreate

logger = logging.getLogger(__name__)

router = APIRouter()


class ModelConfig(BaseModel):
    """æ¨¡å‹é…ç½®"""
    model_config = {"protected_namespaces": ()}
    
    model_name: str = Field(default="gpt-3.5-turbo", description="æ¨¡å‹åç§°")
    temperature: float = Field(default=0.7, ge=0.0, le=2.0, description="æ¸©åº¦å‚æ•°")
    max_tokens: int = Field(default=1000, gt=0, description="æœ€å¤§tokenæ•°")
    api_key: Optional[str] = Field(default=None, description="APIå¯†é’¥")
    base_url: Optional[str] = Field(default="https://api.openai.com/v1", description="APIåŸºç¡€URL")


class ChatRequest(BaseModel):
    """èŠå¤©è¯·æ±‚"""
    session_id: str = Field(description="ä¼šè¯ID")
    content: str = Field(description="æ¶ˆæ¯å†…å®¹")
    ai_model_config: Optional[ModelConfig] = Field(default=None, description="æ¨¡å‹é…ç½®")


class ChatMessage(BaseModel):
    """èŠå¤©æ¶ˆæ¯"""
    role: str = Field(description="è§’è‰²")
    content: str = Field(description="å†…å®¹")


class DirectChatRequest(BaseModel):
    """ç›´æ¥èŠå¤©è¯·æ±‚"""
    session_id: str = Field(description="ä¼šè¯ID")
    messages: list[ChatMessage] = Field(description="æ¶ˆæ¯åˆ—è¡¨")
    ai_model_config: Optional[ModelConfig] = Field(default=None, description="æ¨¡å‹é…ç½®")


# å…¨å±€AIæœåŠ¡å™¨å®ä¾‹ï¼ˆåœ¨å®é™…åº”ç”¨ä¸­åº”è¯¥é€šè¿‡ä¾èµ–æ³¨å…¥ï¼‰
ai_server: Optional[AiServer] = None

# å…¨å±€AIæœåŠ¡å™¨å®ä¾‹ç®¡ç†å™¨ - æŒ‰session_idç®¡ç†å¤šä¸ªä¼šè¯
_session_ai_servers: Dict[str, AiServer] = {}

def set_session_ai_server(session_id: str, server: AiServer) -> None:
    """è®¾ç½®æŒ‡å®šä¼šè¯çš„AIæœåŠ¡å™¨å®ä¾‹"""
    global _session_ai_servers
    _session_ai_servers[session_id] = server
    logger.info(f"ğŸ›‘ [DEBUG] è®¾ç½®ä¼šè¯ {session_id} çš„AIæœåŠ¡å™¨å®ä¾‹")

def get_session_ai_server(session_id: str) -> Optional[AiServer]:
    """è·å–æŒ‡å®šä¼šè¯çš„AIæœåŠ¡å™¨å®ä¾‹"""
    global _session_ai_servers
    server = _session_ai_servers.get(session_id)
    logger.info(f"ğŸ›‘ [DEBUG] è·å–ä¼šè¯ {session_id} çš„AIæœåŠ¡å™¨å®ä¾‹: {server is not None}")
    return server

def remove_session_ai_server(session_id: str) -> None:
    """ç§»é™¤æŒ‡å®šä¼šè¯çš„AIæœåŠ¡å™¨å®ä¾‹"""
    global _session_ai_servers
    if session_id in _session_ai_servers:
        del _session_ai_servers[session_id]
        logger.info(f"ğŸ›‘ [DEBUG] ç§»é™¤ä¼šè¯ {session_id} çš„AIæœåŠ¡å™¨å®ä¾‹")

def get_active_sessions() -> List[str]:
    """è·å–æ‰€æœ‰æ´»è·ƒä¼šè¯IDåˆ—è¡¨"""
    global _session_ai_servers
    return list(_session_ai_servers.keys())


async def load_mcp_configs() -> tuple[Dict[str, Dict[str, Any]], Dict[str, Dict[str, Any]]]:
    """ä»æ•°æ®åº“åŠ è½½MCPé…ç½®"""
    try:
        # è·å–æ‰€æœ‰å¯ç”¨çš„MCPé…ç½®
        db.connection.row_factory = lambda cursor, row: dict(zip([col[0] for col in cursor.description], row))
        configs = await db.fetchall('SELECT * FROM mcp_configs WHERE enabled = 1')
        
        http_servers = {}
        stdio_servers = {}
        
        for config in configs:
            server_name = config['name']
            command = config['command']
            server_type = config.get('type', 'stdio')  # é»˜è®¤ä¸ºstdio
            
            # è§£æargså’Œenv
            try:
                args = json.loads(config.get('args', '[]')) if config.get('args') else []
            except json.JSONDecodeError:
                # å¦‚æœä¸æ˜¯JSONæ ¼å¼ï¼Œå°è¯•æŒ‰é€—å·åˆ†å‰²
                args = config.get('args', '').split(',') if config.get('args') else []
            
            try:
                env = json.loads(config.get('env', '{}')) if config.get('env') else {}
            except json.JSONDecodeError:
                # å¦‚æœä¸æ˜¯JSONæ ¼å¼ï¼Œå°è¯•è§£æä¸ºå­—å…¸å­—ç¬¦ä¸²
                env = {}
            
            # æ ¹æ®typeå­—æ®µåˆ¤æ–­åè®®ç±»å‹
            if server_type == 'http':
                # HTTPåè®®
                http_servers[server_name] = {
                    'url': command,
                    'args': args,
                    'env': env
                }
            else:
                # stdioåè®® - è§£æ command å­—æ®µä¸­çš„ `è„šæœ¬åœ°å€--åˆ«å` æ ¼å¼
                actual_command = command
                alias = server_name  # é»˜è®¤ä½¿ç”¨æœåŠ¡åä½œä¸ºåˆ«å
                
                # æ£€æŸ¥æ˜¯å¦åŒ…å« --åˆ«å æ ¼å¼
                if '--' in command:
                    parts = command.split('--', 1)  # åªåˆ†å‰²ç¬¬ä¸€ä¸ª --
                    actual_command = parts[0].strip()
                    alias = parts[1].strip()
                
                stdio_servers[server_name] = {
                    'command': actual_command,
                    'alias': alias,
                    'args': args,
                    'env': env
                }
        
        logger.info(f"âœ… åŠ è½½MCPé…ç½®å®Œæˆ: HTTPæœåŠ¡å™¨ {len(http_servers)} ä¸ª, stdioæœåŠ¡å™¨ {len(stdio_servers)} ä¸ª")
        return http_servers, stdio_servers
        
    except Exception as e:
        logger.error(f"âŒ åŠ è½½MCPé…ç½®å¤±è´¥: {e}")
        return {}, {}


def get_ai_server() -> AiServer:
    """è·å–AIæœåŠ¡å™¨å®ä¾‹"""
    global ai_server
    if ai_server is None:
        # åˆ›å»ºMCPå·¥å…·æ‰§è¡Œå™¨ï¼ˆæš‚æ—¶ä½¿ç”¨ç¤ºä¾‹é…ç½®ï¼Œå®é™…ä½¿ç”¨æ—¶ä¼šåŠ¨æ€åŠ è½½ï¼‰
        mcp_executor = create_example_mcp_executor()
        
        # åˆ›å»ºAIæœåŠ¡å™¨ï¼ˆç›´æ¥ä½¿ç”¨ models æ¨¡å—ï¼Œä¸éœ€è¦é¢å¤–çš„ database_serviceï¼‰
        ai_server = AiServer(
            database_service=None,  # ä¸å†ä½¿ç”¨ç‹¬ç«‹çš„ database_service
            mcp_tool_execute=mcp_executor
        )
    
    return ai_server


async def get_ai_server_with_mcp_configs() -> AiServer:
    """è·å–å¸¦æœ‰åŠ¨æ€MCPé…ç½®çš„AIæœåŠ¡å™¨å®ä¾‹"""
    try:
        # åŠ è½½MCPé…ç½®
        http_servers, stdio_servers = await load_mcp_configs()
        
        # ä¸å†éœ€è¦ MockDatabaseServiceï¼Œç›´æ¥ä½¿ç”¨ models æ¨¡å—
        
        # åˆ›å»ºæ”¯æŒstdioåè®®çš„MCPå·¥å…·æ‰§è¡Œå™¨
        mcp_executor = McpToolExecute(
            mcp_servers=http_servers,
            stdio_mcp_servers=stdio_servers
        )
        
        # åˆå§‹åŒ–å·¥å…·æ‰§è¡Œå™¨ï¼ˆè¿™ä¼šè°ƒç”¨build_tools()ï¼‰
        await mcp_executor.init()
        
        # è®°å½•å·¥å…·æ„å»ºç»“æœ
        tools_count = len(mcp_executor.get_tools())
        logger.info(f"ğŸ”§ MCPå·¥å…·æ‰§è¡Œå™¨åˆå§‹åŒ–å®Œæˆï¼Œå…±åŠ è½½ {tools_count} ä¸ªå·¥å…·")
        
        # åˆ›å»ºAIæœåŠ¡å™¨ï¼ˆä¸å†éœ€è¦ database_serviceï¼‰
        server = AiServer(
            database_service=None,
            mcp_tool_execute=mcp_executor
        )
        
        return server
        
    except Exception as e:
        logger.error(f"âŒ åˆ›å»ºAIæœåŠ¡å™¨å¤±è´¥: {e}")
        # å›é€€åˆ°ç¤ºä¾‹é…ç½®
        return get_ai_server()


async def create_stream_response(
    session_id: str,
    content: str = None,
    messages: list[Dict[str, Any]] = None,
    model_config: Optional[Dict[str, Any]] = None
) -> AsyncGenerator[str, None]:
    """
    åˆ›å»ºæµå¼å“åº”
    
    Args:
        session_id: ä¼šè¯ID
        content: æ¶ˆæ¯å†…å®¹ï¼ˆç”¨äºsend_messageï¼‰
        messages: æ¶ˆæ¯åˆ—è¡¨ï¼ˆç”¨äºsend_message_directï¼‰
        model_config: æ¨¡å‹é…ç½®
        
    Yields:
        SSEæ ¼å¼çš„æ•°æ®
    """
    try:
        server = await get_ai_server_with_mcp_configs()
        
        # è®¾ç½®ä¸ºæŒ‡å®šä¼šè¯çš„AIæœåŠ¡å™¨å®ä¾‹
        set_session_ai_server(session_id, server)
        
        # åˆ›å»ºäº‹ä»¶é˜Ÿåˆ—
        event_queue = asyncio.Queue()
        
        # è·å–å½“å‰äº‹ä»¶å¾ªç¯ï¼Œç”¨äºå›è°ƒ
        main_loop = asyncio.get_running_loop()
        
        # å®šä¹‰å›è°ƒå‡½æ•°
        def callback(callback_type: str, data: Any):
            try:
                # ä½¿ç”¨çº¿ç¨‹å®‰å…¨çš„æ–¹å¼å°†äº‹ä»¶æ”¾å…¥é˜Ÿåˆ—
                if main_loop and main_loop.is_running():
                    # ä½¿ç”¨call_soon_threadsafeåœ¨ä¸»çº¿ç¨‹ä¸­æ‰§è¡Œ
                    main_loop.call_soon_threadsafe(
                        lambda: asyncio.create_task(event_queue.put((callback_type, data)))
                    )
                else:
                    logger.warning(f"Main loop not available for callback: {callback_type}")
            except Exception as e:
                logger.error(f"Error in callback: {e}")
        
        # å¯åŠ¨AIå¤„ç†ä»»åŠ¡ï¼ˆä½¿ç”¨åŒæ­¥ç‰ˆæœ¬ï¼‰
        if content is not None:
            # ä½¿ç”¨send_message_sync
            ai_task = asyncio.create_task(
                asyncio.to_thread(
                    server.send_message_sync,
                    session_id=session_id,
                    content=content,
                    model_config=model_config,
                    callback=callback
                )
            )
        else:
            # ä½¿ç”¨send_message_direct_sync
            ai_task = asyncio.create_task(
                asyncio.to_thread(
                    server.send_message_direct_sync,
                    session_id=session_id,
                    messages=messages,
                    model_config=model_config,
                    callback=callback
                )
            )
        
        # å°†ä»»åŠ¡æ³¨å†Œåˆ°stream_managerï¼Œä»¥ä¾¿å¯ä»¥è¢«åœæ­¢è¯·æ±‚å–æ¶ˆ
        try:
            await stream_manager.register_stream(session_id, None, ai_task)
        except Exception as e:
            logger.error(f"Failed to register stream: {e}")
            # å³ä½¿æ³¨å†Œå¤±è´¥ï¼Œä¹Ÿç»§ç»­å¤„ç†æµå¼å“åº”
        
        # å¤„ç†äº‹ä»¶æµ
        completed = False
        while not completed:
            try:
                # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¢«å–æ¶ˆ
                if ai_task.cancelled():
                    logger.info(f"Stream for session {session_id} was cancelled")
                    yield f"data: {json.dumps({'type': 'cancelled', 'message': 'Stream was stopped'}, ensure_ascii=False)}\n\n"
                    yield f"data: {json.dumps({'type': 'done'}, ensure_ascii=False)}\n\n"
                    break
                
                # ç­‰å¾…äº‹ä»¶æˆ–ä»»åŠ¡å®Œæˆ
                done, pending = await asyncio.wait(
                    [event_queue.get(), ai_task],
                    return_when=asyncio.FIRST_COMPLETED,
                    timeout=1.0
                )
                
                if ai_task in done:
                    # AIä»»åŠ¡å®Œæˆæˆ–è¢«å–æ¶ˆ
                    logger.info(f"ğŸ¯ AI task completed, processing remaining events. Queue empty: {event_queue.empty()}")
                    
                    # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¢«å–æ¶ˆ
                    if ai_task.cancelled():
                        logger.info(f"AI task for session {session_id} was cancelled")
                        yield f"data: {json.dumps({'type': 'cancelled', 'message': 'Stream was stopped'}, ensure_ascii=False)}\n\n"
                        yield f"data: {json.dumps({'type': 'done'}, ensure_ascii=False)}\n\n"
                        completed = True
                        break
                    
                    # å¤„ç†å‰©ä½™çš„äº‹ä»¶
                    while not event_queue.empty():
                        try:
                            callback_type, data = event_queue.get_nowait()
                            logger.info(f"ğŸ¯ Processing remaining event: {callback_type}")
                            
                            # æ˜ å°„CallbackTypeåˆ°å‰ç«¯æœŸæœ›çš„æ ¼å¼
                            if callback_type == CallbackType.ON_CHUNK:
                                event_data = {
                                    "type": "chunk",
                                    "content": data.get("content", ""),
                                    "accumulated": data.get("accumulated", "")
                                }
                            elif callback_type == CallbackType.ON_TOOL_CALL:
                                event_data = {
                                    "type": "tool_call",
                                    "data": data
                                }
                            elif callback_type == CallbackType.ON_TOOL_RESULT:
                                event_data = {
                                    "type": "tool_result", 
                                    "data": data
                                }
                            elif callback_type == CallbackType.ON_TOOL_STREAM_CHUNK:
                                event_data = {
                                    "type": "tool_stream_chunk",
                                    "data": data
                                }
                            elif callback_type == CallbackType.ON_COMPLETE:
                                # ç¡®ä¿Messageå¯¹è±¡æ­£ç¡®åºåˆ—åŒ–
                                serialized_data = data.copy() if isinstance(data, dict) else {}
                                if "message" in serialized_data and hasattr(serialized_data["message"], "to_dict"):
                                    serialized_data["message"] = serialized_data["message"].to_dict()
                                
                                event_data = {
                                    "type": "complete",
                                    "data": serialized_data
                                }
                                logger.info(f"ğŸ¯ Sending complete event to frontend: {event_data['type']}")
                            elif callback_type == CallbackType.ON_ERROR:
                                event_data = {
                                    "type": "error",
                                    "data": data
                                }
                            else:
                                event_data = {
                                    "type": str(callback_type),
                                    "data": data
                                }
                            
                            yield f"data: {json.dumps(event_data, ensure_ascii=False, default=str)}\n\n"
                        except asyncio.QueueEmpty:
                            break
                    
                    # AIä»»åŠ¡å®Œæˆï¼Œä½†ä¸ç›´æ¥å‘é€doneä¿¡å·
                    # åªæœ‰åœ¨æ”¶åˆ°ON_COMPLETEäº‹ä»¶ä¸”æœ‰finalæ ‡å¿—æ—¶æ‰å‘é€doneä¿¡å·
                    logger.info("ğŸ¯ AI task completed, waiting for final complete event")
                    # ä¸è®¾ç½®completed=Trueï¼Œè®©ç³»ç»Ÿç»§ç»­å¤„ç†äº‹ä»¶
                    break
                
                for task in done:
                    if task != ai_task:
                        # å¤„ç†äº‹ä»¶
                        callback_type, data = await task
                        
                        # æ˜ å°„CallbackTypeåˆ°å‰ç«¯æœŸæœ›çš„æ ¼å¼
                        if callback_type == CallbackType.ON_CHUNK:
                            # å¯¹äºchunkäº‹ä»¶ï¼Œç›´æ¥ä½¿ç”¨dataä¸­çš„å†…å®¹
                            event_data = {
                                "type": "chunk",
                                "content": data.get("content", ""),
                                "accumulated": data.get("accumulated", "")
                            }
                        elif callback_type == CallbackType.ON_TOOL_CALL:
                            event_data = {
                                "type": "tool_call",
                                "data": data
                            }
                        elif callback_type == CallbackType.ON_TOOL_RESULT:
                            event_data = {
                                "type": "tool_result", 
                                "data": data
                            }
                        elif callback_type == CallbackType.ON_TOOL_STREAM_CHUNK:
                            event_data = {
                                "type": "tool_stream_chunk",
                                "data": data
                            }
                        elif callback_type == CallbackType.ON_COMPLETE:
                            # ç¡®ä¿Messageå¯¹è±¡æ­£ç¡®åºåˆ—åŒ–
                            serialized_data = data.copy() if isinstance(data, dict) else {}
                            if "message" in serialized_data and hasattr(serialized_data["message"], "to_dict"):
                                serialized_data["message"] = serialized_data["message"].to_dict()
                            
                            event_data = {
                                "type": "complete",
                                "data": serialized_data
                            }
                        elif callback_type == CallbackType.ON_ERROR:
                            event_data = {
                                "type": "error",
                                "data": data
                            }
                        else:
                            # é»˜è®¤æ ¼å¼
                            event_data = {
                                "type": str(callback_type),
                                "data": data
                            }
                        
                        # å‘é€SSEæ•°æ®
                        yield f"data: {json.dumps(event_data, ensure_ascii=False, default=str)}\n\n"
                        
                        # æ£€æŸ¥æ˜¯å¦å®Œæˆ - åªæœ‰åœ¨çœŸæ­£çš„æœ€ç»ˆå®Œæˆæ—¶æ‰å‘é€doneä¿¡å·
                        if callback_type in [CallbackType.ON_COMPLETE, "complete"]:
                            # æ£€æŸ¥è¿™æ˜¯å¦æ˜¯æœ€ç»ˆçš„å®Œæˆäº‹ä»¶ï¼ˆå¿…é¡»æœ‰finalæ ‡å¿—ï¼‰
                            is_final = False
                            if isinstance(data, dict) and data.get("final") is True:
                                is_final = True
                                logger.info(f"ğŸ¯ Received FINAL complete event, sending done signal")
                            else:
                                logger.info(f"ğŸ¯ Received intermediate complete event (no final flag), continuing...")
                            
                            if is_final:
                                completed = True
                                yield f"data: {json.dumps({'type': 'done'}, ensure_ascii=False)}\n\n"
                                break
                        elif callback_type in [CallbackType.ON_ERROR, "error"]:
                            logger.info(f"ğŸ¯ Received error event, marking as completed")
                            completed = True
                            yield f"data: {json.dumps({'type': 'done'}, ensure_ascii=False)}\n\n"
                            break
                
            except asyncio.TimeoutError:
                # å‘é€å¿ƒè·³
                yield f"data: {json.dumps({'type': 'heartbeat'}, ensure_ascii=False)}\n\n"
                continue
            except Exception as e:
                logger.error(f"Error in stream processing: {e}")
                error_data = {
                    "type": "error",
                    "data": {"error": str(e)}
                }
                yield f"data: {json.dumps(error_data, ensure_ascii=False)}\n\n"
                yield f"data: {json.dumps({'type': 'done'}, ensure_ascii=False)}\n\n"
                break
        
        # ç¡®ä¿ä»»åŠ¡è¢«å–æ¶ˆ
        if not ai_task.done():
            ai_task.cancel()
            try:
                await ai_task
            except asyncio.CancelledError:
                pass
    
    except Exception as e:
        logger.error(f"Error in create_stream_response: {e}")
        
        error_data = {
            "type": "error",
            "data": {"error": str(e)}
        }
        yield f"data: {json.dumps(error_data, ensure_ascii=False)}\n\n"
        yield f"data: {json.dumps({'type': 'done'}, ensure_ascii=False)}\n\n"
    
    finally:
        # ç¡®ä¿åœ¨æ‰€æœ‰æƒ…å†µä¸‹éƒ½å–æ¶ˆæ³¨å†Œæµ
        try:
            await stream_manager.unregister_stream(session_id)
            logger.info(f"ğŸ§¹ Stream {session_id} unregistered successfully")
        except Exception as cleanup_error:
            logger.error(f"Failed to unregister stream during cleanup: {cleanup_error}")
        
        # æ¸…ç†ä¼šè¯AIæœåŠ¡å™¨å®ä¾‹
        try:
            remove_session_ai_server(session_id)
            logger.info(f"ğŸ§¹ Session {session_id} AI server instance cleaned up")
        except Exception as cleanup_error:
            logger.error(f"Failed to cleanup session AI server during cleanup: {cleanup_error}")


@router.post("/chat/stream")
async def chat_stream(request: ChatRequest):
    """
    æµå¼èŠå¤©æ¥å£
    
    æ¥æ”¶ç”¨æˆ·æ¶ˆæ¯å¹¶è¿”å›AIçš„æµå¼å“åº”
    """
    try:
        # è½¬æ¢æ¨¡å‹é…ç½®
        model_config_dict = None
        if request.ai_model_config:
            model_config_dict = {
                "model_name": request.ai_model_config.model_name,
                "temperature": request.ai_model_config.temperature,
                "max_tokens": request.ai_model_config.max_tokens,
                "api_key": request.ai_model_config.api_key,
                "base_url": request.ai_model_config.base_url
            }
        
        # åˆ›å»ºæµå¼å“åº”
        return StreamingResponse(
            create_stream_response(
                session_id=request.session_id,
                content=request.content,
                model_config=model_config_dict
            ),
            media_type="text/plain",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "Content-Type": "text/event-stream"
            }
        )
    
    except Exception as e:
        logger.error(f"Error in chat_stream: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/chat/stream/direct")
async def chat_stream_direct(request: DirectChatRequest):
    """
    ç›´æ¥æµå¼èŠå¤©æ¥å£
    
    æ¥æ”¶æ¶ˆæ¯åˆ—è¡¨å¹¶è¿”å›AIçš„æµå¼å“åº”ï¼ˆä¸ä¿å­˜ç”¨æˆ·æ¶ˆæ¯ï¼‰
    """
    try:
        # è½¬æ¢æ¨¡å‹é…ç½®
        model_config_dict = None
        if request.ai_model_config:
            model_config_dict = {
                "model_name": request.ai_model_config.model_name,
                "temperature": request.ai_model_config.temperature,
                "max_tokens": request.ai_model_config.max_tokens,
                "api_key": request.ai_model_config.api_key,
                "base_url": request.ai_model_config.base_url
            }
        
        # è½¬æ¢æ¶ˆæ¯æ ¼å¼
        messages = [
            {
                "role": msg.role,
                "content": msg.content
            }
            for msg in request.messages
        ]
        
        # åˆ›å»ºæµå¼å“åº”
        return StreamingResponse(
            create_stream_response(
                session_id=request.session_id,
                messages=messages,
                model_config=model_config_dict
            ),
            media_type="text/plain",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "Content-Type": "text/event-stream"
            }
        )
    
    except Exception as e:
        logger.error(f"Error in chat_stream_direct: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/tools")
async def get_available_tools():
    """è·å–å¯ç”¨å·¥å…·åˆ—è¡¨"""
    try:
        server = await get_ai_server_with_mcp_configs()
        tools = server.get_available_tools()
        return {"tools": tools}
    
    except Exception as e:
        logger.error(f"Error getting available tools: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/servers")
async def get_servers_info():
    """è·å–MCPæœåŠ¡å™¨ä¿¡æ¯"""
    try:
        server = await get_ai_server_with_mcp_configs()
        servers = server.get_servers_info()
        return {"servers": servers}
    
    except Exception as e:
        logger.error(f"Error getting servers info: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/chat/abort")
async def abort_chat(request: Request):
    """ä¸­æ­¢å½“å‰èŠå¤©è¯·æ±‚"""
    try:
        # è·å–session_idï¼ˆä»è¯·æ±‚ä½“æˆ–æŸ¥è¯¢å‚æ•°ï¼‰
        body = await request.json() if request.headers.get("content-type") == "application/json" else {}
        session_id = body.get("session_id") or request.query_params.get("session_id")
        
        # ä¸­æ­¢AIæœåŠ¡å™¨è¯·æ±‚
        server = get_ai_server()
        server.abort_request()
        
        # ä¸­æ­¢æµç®¡ç†å™¨ä¸­çš„æµ
        if session_id:
            abort_success = await stream_manager.abort_stream(session_id)
            logger.info(f"ğŸ›‘ Stream abort for session {session_id}: {'success' if abort_success else 'failed'}")
        else:
            logger.warning("âš ï¸ No session_id provided for abort request")
        
        return {"message": "Chat request aborted"}
    
    except Exception as e:
        logger.error(f"Error aborting chat: {e}")
        raise HTTPException(status_code=500, detail=str(e))