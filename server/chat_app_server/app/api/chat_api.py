"""
èŠå¤©API - å¯¹å¤–æä¾›æµå¼èŠå¤©æ¥å£
"""
import json
import logging
import queue
import threading
import time
from typing import Dict, Any, Optional, Generator, List
from datetime import datetime
from fastapi import APIRouter, HTTPException, Request
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field

from ..services.ai_server import AiServer
from ..services.ai_request_handler import AiModelConfig, Message, CallbackType
from ..services.mcp_tool_execute import create_example_mcp_executor, McpToolExecute
from ..models import db
from ..models.message import MessageCreate

logger = logging.getLogger(__name__)

router = APIRouter()


class ModelConfig(BaseModel):
    """æ¨¡å‹é…ç½®"""
    model_config = {"protected_namespaces": ()}
    
    model_name: str = Field(default="gpt-3.5-turbo", description="æ¨¡å‹åç§°")
    temperature: float = Field(default=0.7, ge=0.0, le=2.0, description="æ¸©åº¦å‚æ•°")
    max_tokens: int = Field(default=1000, gt=0, description="æœ€å¤§tokenæ•°")
    api_key: Optional[str] = Field(default=None, description="APIå¯†é’¥")
    base_url: Optional[str] = Field(default="https://api.openai.com/v1", description="APIåŸºç¡€URL")


class ChatRequest(BaseModel):
    """èŠå¤©è¯·æ±‚"""
    session_id: str = Field(description="ä¼šè¯ID")
    content: str = Field(description="æ¶ˆæ¯å†…å®¹")
    ai_model_config: Optional[ModelConfig] = Field(default=None, description="æ¨¡å‹é…ç½®")


class ChatMessage(BaseModel):
    """èŠå¤©æ¶ˆæ¯"""
    role: str = Field(description="è§’è‰²")
    content: str = Field(description="å†…å®¹")


class DirectChatRequest(BaseModel):
    """ç›´æ¥èŠå¤©è¯·æ±‚"""
    session_id: str = Field(description="ä¼šè¯ID")
    messages: list[ChatMessage] = Field(description="æ¶ˆæ¯åˆ—è¡¨")
    ai_model_config: Optional[ModelConfig] = Field(default=None, description="æ¨¡å‹é…ç½®")


# å…¨å±€AIæœåŠ¡å™¨å®ä¾‹ï¼ˆåœ¨å®é™…åº”ç”¨ä¸­åº”è¯¥é€šè¿‡ä¾èµ–æ³¨å…¥ï¼‰
ai_server: Optional[AiServer] = None

# å…¨å±€AIæœåŠ¡å™¨å®ä¾‹ç®¡ç†å™¨ - æŒ‰session_idç®¡ç†å¤šä¸ªä¼šè¯
_session_ai_servers: Dict[str, AiServer] = {}

def set_session_ai_server(session_id: str, server: AiServer) -> None:
    """è®¾ç½®æŒ‡å®šä¼šè¯çš„AIæœåŠ¡å™¨å®ä¾‹"""
    global _session_ai_servers
    _session_ai_servers[session_id] = server
    logger.info(f"ğŸ›‘ [DEBUG] è®¾ç½®ä¼šè¯ {session_id} çš„AIæœåŠ¡å™¨å®ä¾‹")

def get_session_ai_server(session_id: str) -> Optional[AiServer]:
    """è·å–æŒ‡å®šä¼šè¯çš„AIæœåŠ¡å™¨å®ä¾‹"""
    global _session_ai_servers
    server = _session_ai_servers.get(session_id)
    logger.info(f"ğŸ›‘ [DEBUG] è·å–ä¼šè¯ {session_id} çš„AIæœåŠ¡å™¨å®ä¾‹: {server is not None}")
    return server

def remove_session_ai_server(session_id: str) -> None:
    """ç§»é™¤æŒ‡å®šä¼šè¯çš„AIæœåŠ¡å™¨å®ä¾‹"""
    global _session_ai_servers
    if session_id in _session_ai_servers:
        del _session_ai_servers[session_id]
        logger.info(f"ğŸ›‘ [DEBUG] ç§»é™¤ä¼šè¯ {session_id} çš„AIæœåŠ¡å™¨å®ä¾‹")

def get_active_sessions() -> List[str]:
    """è·å–æ‰€æœ‰æ´»è·ƒä¼šè¯IDåˆ—è¡¨"""
    global _session_ai_servers
    return list(_session_ai_servers.keys())


def load_mcp_configs_sync() -> tuple[Dict[str, Dict[str, Any]], Dict[str, Dict[str, Any]]]:
    """ä»æ•°æ®åº“åŠ è½½MCPé…ç½®ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰"""
    try:
        # è·å–æ‰€æœ‰å¯ç”¨çš„MCPé…ç½®
        configs = db.fetchall_sync('SELECT * FROM mcp_configs WHERE enabled = 1')
        
        http_servers = {}
        stdio_servers = {}
        
        for config in configs:
            server_name = config['name']
            command = config['command']
            server_type = config.get('type', 'stdio')  # é»˜è®¤ä¸ºstdio
            
            # è§£æargså’Œenv
            try:
                args = json.loads(config.get('args', '[]')) if config.get('args') else []
            except json.JSONDecodeError:
                # å¦‚æœä¸æ˜¯JSONæ ¼å¼ï¼Œå°è¯•æŒ‰é€—å·åˆ†å‰²
                args = config.get('args', '').split(',') if config.get('args') else []
            
            try:
                env = json.loads(config.get('env', '{}')) if config.get('env') else {}
            except json.JSONDecodeError:
                # å¦‚æœä¸æ˜¯JSONæ ¼å¼ï¼Œå°è¯•è§£æä¸ºå­—å…¸å­—ç¬¦ä¸²
                env = {}
            
            # æ ¹æ®typeå­—æ®µåˆ¤æ–­åè®®ç±»å‹
            if server_type == 'http':
                # HTTPåè®®
                http_servers[server_name] = {
                    'url': command,
                    'args': args,
                    'env': env
                }
            else:
                # stdioåè®® - è§£æ command å­—æ®µä¸­çš„ `è„šæœ¬åœ°å€--åˆ«å` æ ¼å¼
                actual_command = command
                alias = server_name  # é»˜è®¤ä½¿ç”¨æœåŠ¡åä½œä¸ºåˆ«å
                
                # æ£€æŸ¥æ˜¯å¦åŒ…å« --åˆ«å æ ¼å¼
                if '--' in command:
                    parts = command.split('--', 1)  # åªåˆ†å‰²ç¬¬ä¸€ä¸ª --
                    actual_command = parts[0].strip()
                    alias = parts[1].strip()
                
                stdio_servers[server_name] = {
                    'command': actual_command,
                    'alias': alias,
                    'args': args,
                    'env': env
                }
        
        logger.info(f"âœ… åŠ è½½MCPé…ç½®å®Œæˆ: HTTPæœåŠ¡å™¨ {len(http_servers)} ä¸ª, stdioæœåŠ¡å™¨ {len(stdio_servers)} ä¸ª")
        return http_servers, stdio_servers
        
    except Exception as e:
        logger.error(f"âŒ åŠ è½½MCPé…ç½®å¤±è´¥: {e}")
        return {}, {}


async def load_mcp_configs() -> tuple[Dict[str, Dict[str, Any]], Dict[str, Dict[str, Any]]]:
    """ä»æ•°æ®åº“åŠ è½½MCPé…ç½®ï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼Œä¿æŒå…¼å®¹æ€§ï¼‰"""
    try:
        # è·å–æ‰€æœ‰å¯ç”¨çš„MCPé…ç½®
        db.connection.row_factory = lambda cursor, row: dict(zip([col[0] for col in cursor.description], row))
        configs = await db.fetchall('SELECT * FROM mcp_configs WHERE enabled = 1')
        
        http_servers = {}
        stdio_servers = {}
        
        for config in configs:
            server_name = config['name']
            command = config['command']
            server_type = config.get('type', 'stdio')  # é»˜è®¤ä¸ºstdio
            
            # è§£æargså’Œenv
            try:
                args = json.loads(config.get('args', '[]')) if config.get('args') else []
            except json.JSONDecodeError:
                # å¦‚æœä¸æ˜¯JSONæ ¼å¼ï¼Œå°è¯•æŒ‰é€—å·åˆ†å‰²
                args = config.get('args', '').split(',') if config.get('args') else []
            
            try:
                env = json.loads(config.get('env', '{}')) if config.get('env') else {}
            except json.JSONDecodeError:
                # å¦‚æœä¸æ˜¯JSONæ ¼å¼ï¼Œå°è¯•è§£æä¸ºå­—å…¸å­—ç¬¦ä¸²
                env = {}
            
            # æ ¹æ®typeå­—æ®µåˆ¤æ–­åè®®ç±»å‹
            if server_type == 'http':
                # HTTPåè®®
                http_servers[server_name] = {
                    'url': command,
                    'args': args,
                    'env': env
                }
            else:
                # stdioåè®® - è§£æ command å­—æ®µä¸­çš„ `è„šæœ¬åœ°å€--åˆ«å` æ ¼å¼
                actual_command = command
                alias = server_name  # é»˜è®¤ä½¿ç”¨æœåŠ¡åä½œä¸ºåˆ«å
                
                # æ£€æŸ¥æ˜¯å¦åŒ…å« --åˆ«å æ ¼å¼
                if '--' in command:
                    parts = command.split('--', 1)  # åªåˆ†å‰²ç¬¬ä¸€ä¸ª --
                    actual_command = parts[0].strip()
                    alias = parts[1].strip()
                
                stdio_servers[server_name] = {
                    'command': actual_command,
                    'alias': alias,
                    'args': args,
                    'env': env
                }
        
        logger.info(f"âœ… åŠ è½½MCPé…ç½®å®Œæˆ: HTTPæœåŠ¡å™¨ {len(http_servers)} ä¸ª, stdioæœåŠ¡å™¨ {len(stdio_servers)} ä¸ª")
        return http_servers, stdio_servers
        
    except Exception as e:
        logger.error(f"âŒ åŠ è½½MCPé…ç½®å¤±è´¥: {e}")
        return {}, {}


def get_ai_server() -> AiServer:
    """è·å–AIæœåŠ¡å™¨å®ä¾‹"""
    global ai_server
    if ai_server is None:
        # åˆ›å»ºMCPå·¥å…·æ‰§è¡Œå™¨ï¼ˆæš‚æ—¶ä½¿ç”¨ç¤ºä¾‹é…ç½®ï¼Œå®é™…ä½¿ç”¨æ—¶ä¼šåŠ¨æ€åŠ è½½ï¼‰
        mcp_executor = create_example_mcp_executor()
        
        # åˆ›å»ºAIæœåŠ¡å™¨ï¼ˆç›´æ¥ä½¿ç”¨ models æ¨¡å—ï¼Œä¸éœ€è¦é¢å¤–çš„ database_serviceï¼‰
        ai_server = AiServer(
            database_service=None,  # ä¸å†ä½¿ç”¨ç‹¬ç«‹çš„ database_service
            mcp_tool_execute=mcp_executor
        )
    
    return ai_server


def get_ai_server_with_mcp_configs_sync() -> AiServer:
    """è·å–å¸¦æœ‰åŠ¨æ€MCPé…ç½®çš„AIæœåŠ¡å™¨å®ä¾‹ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰"""
    try:
        # åŠ è½½MCPé…ç½®
        http_servers, stdio_servers = load_mcp_configs_sync()
        
        # åˆ›å»ºæ”¯æŒstdioåè®®çš„MCPå·¥å…·æ‰§è¡Œå™¨
        mcp_executor = McpToolExecute(
            mcp_servers=http_servers,
            stdio_mcp_servers=stdio_servers
        )
        
        # åˆå§‹åŒ–å·¥å…·æ‰§è¡Œå™¨ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰
        mcp_executor.init_sync()
        
        # è®°å½•å·¥å…·æ„å»ºç»“æœ
        tools_count = len(mcp_executor.get_tools())
        logger.info(f"ğŸ”§ MCPå·¥å…·æ‰§è¡Œå™¨åˆå§‹åŒ–å®Œæˆï¼Œå…±åŠ è½½ {tools_count} ä¸ªå·¥å…·")
        
        # åˆ›å»ºAIæœåŠ¡å™¨
        server = AiServer(
            database_service=None,
            mcp_tool_execute=mcp_executor
        )
        
        return server
        
    except Exception as e:
        logger.error(f"âŒ åˆ›å»ºAIæœåŠ¡å™¨å¤±è´¥: {e}")
        # å›é€€åˆ°ç¤ºä¾‹é…ç½®
        return get_ai_server()


async def get_ai_server_with_mcp_configs() -> AiServer:
    """è·å–å¸¦æœ‰åŠ¨æ€MCPé…ç½®çš„AIæœåŠ¡å™¨å®ä¾‹ï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼Œä¿æŒå…¼å®¹æ€§ï¼‰"""
    try:
        # åŠ è½½MCPé…ç½®
        http_servers, stdio_servers = await load_mcp_configs()
        
        # ä¸å†éœ€è¦ MockDatabaseServiceï¼Œç›´æ¥ä½¿ç”¨ models æ¨¡å—
        
        # åˆ›å»ºæ”¯æŒstdioåè®®çš„MCPå·¥å…·æ‰§è¡Œå™¨
        mcp_executor = McpToolExecute(
            mcp_servers=http_servers,
            stdio_mcp_servers=stdio_servers
        )
        
        # åˆå§‹åŒ–å·¥å…·æ‰§è¡Œå™¨ï¼ˆè¿™ä¼šè°ƒç”¨build_tools()ï¼‰
        await mcp_executor.init()
        
        # è®°å½•å·¥å…·æ„å»ºç»“æœ
        tools_count = len(mcp_executor.get_tools())
        logger.info(f"ğŸ”§ MCPå·¥å…·æ‰§è¡Œå™¨åˆå§‹åŒ–å®Œæˆï¼Œå…±åŠ è½½ {tools_count} ä¸ªå·¥å…·")
        
        # åˆ›å»ºAIæœåŠ¡å™¨ï¼ˆä¸å†éœ€è¦ database_serviceï¼‰
        server = AiServer(
            database_service=None,
            mcp_tool_execute=mcp_executor
        )
        
        return server
        
    except Exception as e:
        logger.error(f"âŒ åˆ›å»ºAIæœåŠ¡å™¨å¤±è´¥: {e}")
        # å›é€€åˆ°ç¤ºä¾‹é…ç½®
        return get_ai_server()


def create_stream_response(
    session_id: str,
    content: str = None,
    messages: list[Dict[str, Any]] = None,
    model_config: Optional[Dict[str, Any]] = None
) -> Generator[str, None, None]:
    """
    åˆ›å»ºæµå¼å“åº”ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰
    
    Args:
        session_id: ä¼šè¯ID
        content: æ¶ˆæ¯å†…å®¹ï¼ˆç”¨äºsend_messageï¼‰
        messages: æ¶ˆæ¯åˆ—è¡¨ï¼ˆç”¨äºsend_message_directï¼‰
        model_config: æ¨¡å‹é…ç½®
        
    Yields:
        SSEæ ¼å¼çš„æ•°æ®
    """
    try:
        # è·å–AIæœåŠ¡å™¨å®ä¾‹ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰
        server = get_ai_server_with_mcp_configs_sync()
        
        # è®¾ç½®ä¸ºæŒ‡å®šä¼šè¯çš„AIæœåŠ¡å™¨å®ä¾‹
        set_session_ai_server(session_id, server)
        
        # åˆ›å»ºçº¿ç¨‹å®‰å…¨çš„äº‹ä»¶é˜Ÿåˆ—
        event_queue = queue.Queue()
        
        # å®šä¹‰å›è°ƒå‡½æ•°
        def callback(callback_type: str, data: Any):
            try:
                # å°†äº‹ä»¶æ”¾å…¥é˜Ÿåˆ—
                event_queue.put((callback_type, data))
            except Exception as e:
                logger.error(f"Error in callback: {e}")
        
        # åˆ›å»ºä¸€ä¸ªæ ‡å¿—æ¥æ§åˆ¶AIå¤„ç†çº¿ç¨‹
        ai_completed = threading.Event()
        ai_error = None
        
        # å¯åŠ¨AIå¤„ç†çº¿ç¨‹
        def ai_worker():
            nonlocal ai_error
            try:
                if content is not None:
                    # ä½¿ç”¨send_message_sync
                    server.send_message_sync(
                        session_id=session_id,
                        content=content,
                        model_config=model_config,
                        callback=callback
                    )
                else:
                    # ä½¿ç”¨send_message_direct_sync
                    server.send_message_direct_sync(
                        session_id=session_id,
                        messages=messages,
                        model_config=model_config,
                        callback=callback
                    )
            except Exception as e:
                ai_error = e
                logger.error(f"Error in AI worker: {e}")
            finally:
                ai_completed.set()
        
        # å¯åŠ¨AIå¤„ç†çº¿ç¨‹
        ai_thread = threading.Thread(target=ai_worker, daemon=True)
        ai_thread.start()
        
        # å¤„ç†äº‹ä»¶æµ
        completed = False
        last_heartbeat = time.time()
        heartbeat_interval = 30  # 30ç§’å¿ƒè·³é—´éš”
        
        while not completed:
            try:
                # æ£€æŸ¥æ˜¯å¦æœ‰æ–°äº‹ä»¶
                try:
                    callback_type, data = event_queue.get(timeout=1.0)
                    
                    # æ˜ å°„CallbackTypeåˆ°å‰ç«¯æœŸæœ›çš„æ ¼å¼
                    if callback_type == CallbackType.ON_CHUNK:
                        event_data = {
                            "type": "chunk",
                            "content": data.get("content", ""),
                            "accumulated": data.get("accumulated", "")
                        }
                    elif callback_type == CallbackType.ON_TOOL_CALL:
                        event_data = {
                            "type": "tool_call",
                            "data": data
                        }
                    elif callback_type == CallbackType.ON_TOOL_RESULT:
                        event_data = {
                            "type": "tool_result", 
                            "data": data
                        }
                    elif callback_type == CallbackType.ON_TOOL_STREAM_CHUNK:
                        event_data = {
                            "type": "tool_stream_chunk",
                            "data": data
                        }
                    elif callback_type == CallbackType.ON_COMPLETE:
                        # ç¡®ä¿Messageå¯¹è±¡æ­£ç¡®åºåˆ—åŒ–
                        serialized_data = data.copy() if isinstance(data, dict) else {}
                        if "message" in serialized_data and hasattr(serialized_data["message"], "to_dict"):
                            serialized_data["message"] = serialized_data["message"].to_dict()
                        
                        event_data = {
                            "type": "complete",
                            "data": serialized_data
                        }
                        logger.info(f"ğŸ¯ Sending complete event to frontend: {event_data['type']}")
                    elif callback_type == CallbackType.ON_ERROR:
                        event_data = {
                            "type": "error",
                            "data": data
                        }
                    else:
                        event_data = {
                            "type": str(callback_type),
                            "data": data
                        }
                    
                    # å‘é€SSEæ•°æ®
                    yield f"data: {json.dumps(event_data, ensure_ascii=False, default=str)}\n\n"
                    
                    # æ£€æŸ¥æ˜¯å¦å®Œæˆ
                    if callback_type in [CallbackType.ON_COMPLETE, "complete"]:
                        # æ£€æŸ¥è¿™æ˜¯å¦æ˜¯æœ€ç»ˆçš„å®Œæˆäº‹ä»¶ï¼ˆå¿…é¡»æœ‰finalæ ‡å¿—ï¼‰
                        is_final = False
                        if isinstance(data, dict) and data.get("final") is True:
                            is_final = True
                            logger.info(f"ğŸ¯ Received FINAL complete event, sending done signal")
                        else:
                            logger.info(f"ğŸ¯ Received intermediate complete event (no final flag), continuing...")
                        
                        if is_final:
                            completed = True
                            yield f"data: {json.dumps({'type': 'done'}, ensure_ascii=False)}\n\n"
                            break
                    elif callback_type in [CallbackType.ON_ERROR, "error"]:
                        logger.info(f"ğŸ¯ Received error event, marking as completed")
                        completed = True
                        yield f"data: {json.dumps({'type': 'done'}, ensure_ascii=False)}\n\n"
                        break
                        
                except queue.Empty:
                    # æ²¡æœ‰æ–°äº‹ä»¶ï¼Œæ£€æŸ¥AIçº¿ç¨‹æ˜¯å¦å®Œæˆ
                    if ai_completed.is_set():
                        # AIçº¿ç¨‹å®Œæˆï¼Œå¤„ç†å‰©ä½™äº‹ä»¶
                        logger.info(f"ğŸ¯ AI thread completed, processing remaining events")
                        
                        # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
                        if ai_error:
                            error_data = {
                                "type": "error",
                                "data": {"error": str(ai_error)}
                            }
                            yield f"data: {json.dumps(error_data, ensure_ascii=False)}\n\n"
                            yield f"data: {json.dumps({'type': 'done'}, ensure_ascii=False)}\n\n"
                            break
                        
                        # å¤„ç†å‰©ä½™çš„äº‹ä»¶
                        remaining_events = []
                        while True:
                            try:
                                remaining_events.append(event_queue.get_nowait())
                            except queue.Empty:
                                break
                        
                        if remaining_events:
                            logger.info(f"ğŸ¯ Processing {len(remaining_events)} remaining events")
                            for callback_type, data in remaining_events:
                                # æ˜ å°„CallbackTypeåˆ°å‰ç«¯æœŸæœ›çš„æ ¼å¼
                                if callback_type == CallbackType.ON_CHUNK:
                                    event_data = {
                                        "type": "chunk",
                                        "content": data.get("content", ""),
                                        "accumulated": data.get("accumulated", "")
                                    }
                                elif callback_type == CallbackType.ON_TOOL_CALL:
                                    event_data = {
                                        "type": "tool_call",
                                        "data": data
                                    }
                                elif callback_type == CallbackType.ON_TOOL_RESULT:
                                    event_data = {
                                        "type": "tool_result", 
                                        "data": data
                                    }
                                elif callback_type == CallbackType.ON_TOOL_STREAM_CHUNK:
                                    event_data = {
                                        "type": "tool_stream_chunk",
                                        "data": data
                                    }
                                elif callback_type == CallbackType.ON_COMPLETE:
                                    # ç¡®ä¿Messageå¯¹è±¡æ­£ç¡®åºåˆ—åŒ–
                                    serialized_data = data.copy() if isinstance(data, dict) else {}
                                    if "message" in serialized_data and hasattr(serialized_data["message"], "to_dict"):
                                        serialized_data["message"] = serialized_data["message"].to_dict()
                                    
                                    event_data = {
                                        "type": "complete",
                                        "data": serialized_data
                                    }
                                    logger.info(f"ğŸ¯ Sending complete event to frontend: {event_data['type']}")
                                elif callback_type == CallbackType.ON_ERROR:
                                    event_data = {
                                        "type": "error",
                                        "data": data
                                    }
                                else:
                                    event_data = {
                                        "type": str(callback_type),
                                        "data": data
                                    }
                                
                                yield f"data: {json.dumps(event_data, ensure_ascii=False, default=str)}\n\n"
                                
                                # æ£€æŸ¥æ˜¯å¦å®Œæˆ
                                if callback_type in [CallbackType.ON_COMPLETE, "complete"]:
                                    # æ£€æŸ¥è¿™æ˜¯å¦æ˜¯æœ€ç»ˆçš„å®Œæˆäº‹ä»¶
                                    is_final = False
                                    if isinstance(data, dict) and data.get("final") is True:
                                        is_final = True
                                        logger.info(f"ğŸ¯ Received FINAL complete event, sending done signal")
                                    
                                    if is_final:
                                        completed = True
                                        yield f"data: {json.dumps({'type': 'done'}, ensure_ascii=False)}\n\n"
                                        break
                                elif callback_type in [CallbackType.ON_ERROR, "error"]:
                                    logger.info(f"ğŸ¯ Received error event, marking as completed")
                                    completed = True
                                    yield f"data: {json.dumps({'type': 'done'}, ensure_ascii=False)}\n\n"
                                    break
                        
                        # å¦‚æœæ²¡æœ‰æ”¶åˆ°å®Œæˆäº‹ä»¶ï¼Œå‘é€doneä¿¡å·
                        if not completed:
                            logger.info("ğŸ¯ AI thread completed but no final complete event received, sending done signal")
                            yield f"data: {json.dumps({'type': 'done'}, ensure_ascii=False)}\n\n"
                            completed = True
                    else:
                        # å‘é€å¿ƒè·³ï¼ˆå¦‚æœéœ€è¦ï¼‰
                        current_time = time.time()
                        if current_time - last_heartbeat > heartbeat_interval:
                            yield f"data: {json.dumps({'type': 'heartbeat'}, ensure_ascii=False)}\n\n"
                            last_heartbeat = current_time
                
            except Exception as e:
                logger.error(f"Error in stream processing: {e}")
                error_data = {
                    "type": "error",
                    "data": {"error": str(e)}
                }
                yield f"data: {json.dumps(error_data, ensure_ascii=False)}\n\n"
                yield f"data: {json.dumps({'type': 'done'}, ensure_ascii=False)}\n\n"
                break
        
        # ç­‰å¾…AIçº¿ç¨‹å®Œæˆ
        if ai_thread.is_alive():
            ai_thread.join(timeout=5.0)
    
    except Exception as e:
        logger.error(f"Error in create_stream_response: {e}")
        
        error_data = {
            "type": "error",
            "data": {"error": str(e)}
        }
        yield f"data: {json.dumps(error_data, ensure_ascii=False)}\n\n"
        yield f"data: {json.dumps({'type': 'done'}, ensure_ascii=False)}\n\n"
    
    finally:
        # æ¸…ç†ä¼šè¯AIæœåŠ¡å™¨å®ä¾‹
        try:
            remove_session_ai_server(session_id)
            logger.info(f"ğŸ§¹ Session {session_id} AI server instance cleaned up")
        except Exception as cleanup_error:
            logger.error(f"Failed to cleanup session AI server during cleanup: {cleanup_error}")


@router.post("/chat/stream")
async def chat_stream(request: ChatRequest):
    """
    æµå¼èŠå¤©æ¥å£
    
    æ¥æ”¶ç”¨æˆ·æ¶ˆæ¯å¹¶è¿”å›AIçš„æµå¼å“åº”
    """
    try:
        # è½¬æ¢æ¨¡å‹é…ç½®
        model_config_dict = None
        if request.ai_model_config:
            model_config_dict = {
                "model_name": request.ai_model_config.model_name,
                "temperature": request.ai_model_config.temperature,
                "max_tokens": request.ai_model_config.max_tokens,
                "api_key": request.ai_model_config.api_key,
                "base_url": request.ai_model_config.base_url
            }
        
        # åˆ›å»ºæµå¼å“åº”
        return StreamingResponse(
            create_stream_response(
                session_id=request.session_id,
                content=request.content,
                model_config=model_config_dict
            ),
            media_type="text/plain",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "Content-Type": "text/event-stream"
            }
        )
    
    except Exception as e:
        logger.error(f"Error in chat_stream: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/chat/stream/direct")
async def chat_stream_direct(request: DirectChatRequest):
    """
    ç›´æ¥æµå¼èŠå¤©æ¥å£
    
    æ¥æ”¶æ¶ˆæ¯åˆ—è¡¨å¹¶è¿”å›AIçš„æµå¼å“åº”ï¼ˆä¸ä¿å­˜ç”¨æˆ·æ¶ˆæ¯ï¼‰
    """
    try:
        # è½¬æ¢æ¨¡å‹é…ç½®
        model_config_dict = None
        if request.ai_model_config:
            model_config_dict = {
                "model_name": request.ai_model_config.model_name,
                "temperature": request.ai_model_config.temperature,
                "max_tokens": request.ai_model_config.max_tokens,
                "api_key": request.ai_model_config.api_key,
                "base_url": request.ai_model_config.base_url
            }
        
        # è½¬æ¢æ¶ˆæ¯æ ¼å¼
        messages = [
            {
                "role": msg.role,
                "content": msg.content
            }
            for msg in request.messages
        ]
        
        # åˆ›å»ºæµå¼å“åº”
        return StreamingResponse(
            create_stream_response(
                session_id=request.session_id,
                messages=messages,
                model_config=model_config_dict
            ),
            media_type="text/plain",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "Content-Type": "text/event-stream"
            }
        )
    
    except Exception as e:
        logger.error(f"Error in chat_stream_direct: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/tools")
async def get_available_tools():
    """è·å–å¯ç”¨å·¥å…·åˆ—è¡¨"""
    try:
        server = get_ai_server_with_mcp_configs_sync()
        tools = server.get_available_tools()
        return {"tools": tools}
    
    except Exception as e:
        logger.error(f"Error getting available tools: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/servers")
async def get_servers_info():
    """è·å–MCPæœåŠ¡å™¨ä¿¡æ¯"""
    try:
        server = get_ai_server_with_mcp_configs_sync()
        servers = server.get_servers_info()
        return {"servers": servers}
    
    except Exception as e:
        logger.error(f"Error getting servers info: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/chat/abort")
async def abort_chat(request: Request):
    """ä¸­æ­¢å½“å‰èŠå¤©è¯·æ±‚"""
    try:
        # è·å–session_idï¼ˆä»è¯·æ±‚ä½“æˆ–æŸ¥è¯¢å‚æ•°ï¼‰
        body = await request.json() if request.headers.get("content-type") == "application/json" else {}
        session_id = body.get("session_id") or request.query_params.get("session_id")
        
        # ä¸­æ­¢AIæœåŠ¡å™¨è¯·æ±‚
        server = get_ai_server()
        server.abort_request()
        
        # æ³¨æ„ï¼šç”±äºæˆ‘ä»¬å·²ç»ç§»é™¤äº†stream_managerï¼Œè¿™é‡Œåªéœ€è¦ä¸­æ­¢AIæœåŠ¡å™¨è¯·æ±‚
        logger.info(f"ğŸ›‘ Chat abort request for session {session_id}")
        
        return {"message": "Chat request aborted"}
    
    except Exception as e:
        logger.error(f"Error aborting chat: {e}")
        raise HTTPException(status_code=500, detail=str(e))