#!/usr/bin/env python3
"""
Chat API v2 - ä½¿ç”¨ v2 ç‰ˆæœ¬çš„æœåŠ¡
ä¸“é—¨ä¸º v2 ç‰ˆæœ¬çš„ AiServer è®¾è®¡çš„è·¯ç”±
"""

import asyncio
import json
import logging
import os
import queue
import threading
import time
from typing import Dict, Any, Optional, Generator, List
from datetime import datetime
from fastapi import APIRouter, HTTPException, Request
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field

from ..services.v2.ai_server import AiServer
from ..services.v2.mcp_tool_execute import McpToolExecute
from ..models.database_factory import get_database
from ..models.message import MessageCreate
from ..models.config import McpConfigCreate

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/v2")


# ===== æ•°æ®æ¨¡å‹ =====

class ModelConfigV2(BaseModel):
    """v2 ç‰ˆæœ¬çš„æ¨¡å‹é…ç½®"""
    model_config = {"protected_namespaces": ()}
    
    model_name: str = Field(default="gpt-4", description="æ¨¡å‹åç§°")
    temperature: float = Field(default=0.7, ge=0.0, le=2.0, description="æ¸©åº¦å‚æ•°")
    max_tokens: int = Field(default=4000, gt=0, description="æœ€å¤§tokenæ•°")
    use_tools: bool = Field(default=True, description="æ˜¯å¦ä½¿ç”¨å·¥å…·")
    api_key: Optional[str] = Field(default=None, description="APIå¯†é’¥")
    base_url: Optional[str] = Field(default=None, description="APIåŸºç¡€URL")


class ChatRequestV2(BaseModel):
    """v2 ç‰ˆæœ¬çš„èŠå¤©è¯·æ±‚"""
    session_id: str = Field(description="ä¼šè¯ID")
    content: str = Field(description="æ¶ˆæ¯å†…å®¹")
    ai_model_config: Optional[ModelConfigV2] = Field(default=None, description="æ¨¡å‹é…ç½®")
    user_id: Optional[str] = Field(default=None, description="ç”¨æˆ·IDï¼Œç”¨äºæŒ‰ç”¨æˆ·åŠ è½½MCPé…ç½®")


class ChatMessageV2(BaseModel):
    """v2 ç‰ˆæœ¬çš„èŠå¤©æ¶ˆæ¯"""
    role: str = Field(description="è§’è‰²")
    content: str = Field(description="å†…å®¹")


class DirectChatRequestV2(BaseModel):
    """v2 ç‰ˆæœ¬çš„ç›´æ¥èŠå¤©è¯·æ±‚"""
    session_id: str = Field(description="ä¼šè¯ID")
    messages: List[ChatMessageV2] = Field(description="æ¶ˆæ¯åˆ—è¡¨")
    ai_model_config: Optional[ModelConfigV2] = Field(default=None, description="æ¨¡å‹é…ç½®")


# ===== å…¨å±€å˜é‡ =====

# å…¨å±€ AI æœåŠ¡å™¨å®ä¾‹
ai_server: Optional[AiServer] = None

# ä¼šè¯çº§åˆ«çš„ AI æœåŠ¡å™¨å®ä¾‹
_session_ai_servers: Dict[str, AiServer] = {}


# ===== æœåŠ¡å™¨ç®¡ç†å‡½æ•° =====

def set_session_ai_server(session_id: str, server: AiServer) -> None:
    """è®¾ç½®ä¼šè¯çº§åˆ«çš„AIæœåŠ¡å™¨"""
    global _session_ai_servers
    _session_ai_servers[session_id] = server


def get_session_ai_server(session_id: str) -> Optional[AiServer]:
    """è·å–ä¼šè¯çº§åˆ«çš„AIæœåŠ¡å™¨"""
    global _session_ai_servers
    return _session_ai_servers.get(session_id)


def remove_session_ai_server(session_id: str) -> None:
    """ç§»é™¤ä¼šè¯çº§åˆ«çš„AIæœåŠ¡å™¨"""
    global _session_ai_servers
    if session_id in _session_ai_servers:
        del _session_ai_servers[session_id]


def get_active_sessions() -> List[str]:
    """è·å–æ´»è·ƒä¼šè¯åˆ—è¡¨"""
    global _session_ai_servers
    return list(_session_ai_servers.keys())


# ===== MCP é…ç½®åŠ è½½ =====

def load_mcp_configs_sync(user_id: Optional[str] = None) -> tuple[Dict[str, Dict[str, Any]], Dict[str, Dict[str, Any]]]:
    """åŒæ­¥åŠ è½½MCPé…ç½®ï¼ˆæ”¯æŒæŒ‰ç”¨æˆ·è¿‡æ»¤ï¼‰"""
    try:
        # ä½¿ç”¨æ¨¡å‹çš„ç»Ÿä¸€æ–¹æ³•æŒ‰éœ€åŠ è½½é…ç½®ï¼ˆå¼‚æ­¥æ–¹æ³•åœ¨æ­¤åŒæ­¥è°ƒç”¨ï¼‰
        configs = asyncio.run(McpConfigCreate.get_all(user_id=user_id))

        # ä»…ä¿ç•™å¯ç”¨çš„é…ç½®
        configs = [c for c in configs if c.get('enabled')]
        
        http_servers = {}
        stdio_servers = {}
        
        for config in configs:
            server_name = config['name']
            command = config['command']
            server_type = config.get('type', 'stdio')
            
            # è§£æargså’Œenv
            try:
                args = json.loads(config.get('args', '[]')) if isinstance(config.get('args'), str) else (config.get('args') or [])
            except json.JSONDecodeError:
                args = config.get('args', []) or []
            
            try:
                env = json.loads(config.get('env', '{}')) if isinstance(config.get('env'), str) else (config.get('env') or {})
            except json.JSONDecodeError:
                env = config.get('env', {}) or {}
            
            if server_type == 'http':
                http_servers[server_name] = {
                    'url': command,
                    'args': args,
                    'env': env
                }
            else:
                actual_command = command
                alias = server_name
                
                if '--' in command:
                    parts = command.split('--', 1)
                    actual_command = parts[0].strip()
                    alias = parts[1].strip()
                
                stdio_servers[server_name] = {
                    'command': actual_command,
                    'alias': alias,
                    'args': args,
                    'env': env
                }
        
        logger.info(f"âœ… v2 åŠ è½½MCPé…ç½®å®Œæˆ: HTTPæœåŠ¡å™¨ {len(http_servers)} ä¸ª, stdioæœåŠ¡å™¨ {len(stdio_servers)} ä¸ª")
        return http_servers, stdio_servers
        
    except Exception as e:
        logger.error(f"âŒ v2 åŠ è½½MCPé…ç½®å¤±è´¥: {e}")
        return {}, {}


# ===== AI æœåŠ¡å™¨åˆ›å»ºå‡½æ•° =====

def get_ai_server_v2() -> AiServer:
    """è·å– v2 ç‰ˆæœ¬çš„ AI æœåŠ¡å™¨å®ä¾‹"""
    global ai_server
    
    if ai_server is None:
        # è·å– OpenAI API å¯†é’¥
        openai_api_key = os.getenv("OPENAI_API_KEY")
        if not openai_api_key:
            raise HTTPException(status_code=500, detail="OPENAI_API_KEY ç¯å¢ƒå˜é‡æœªè®¾ç½®")
        
        # åŠ è½½ MCP é…ç½®
        http_servers, stdio_servers = load_mcp_configs_sync()
        
        # å°† HTTP é…ç½®è½¬æ¢ä¸º mcp_servers æ ¼å¼
        mcp_servers = []
        for name, config in http_servers.items():
            mcp_servers.append({
                "name": name,
                "url": config["url"]
            })
        
        # å°† stdio é…ç½®è½¬æ¢ä¸º stdio_mcp_servers æ ¼å¼
        stdio_mcp_servers = []
        for name, config in stdio_servers.items():
            stdio_mcp_servers.append({
                "name": name,
                "command": config["command"],
                "alias": config["alias"],
                "args": config.get("args", []),
                "env": config.get("env", {})
            })
        
        # è®¾ç½®é…ç½®ç›®å½•
        config_dir = os.path.expanduser("~/.mcp_framework/configs")
        
        # åˆ›å»º MCP æ‰§è¡Œå™¨
        mcp_tool_execute = McpToolExecute(
            mcp_servers=mcp_servers,
            stdio_mcp_servers=stdio_mcp_servers,
            config_dir=config_dir
        )
        mcp_tool_execute.init()  # åˆå§‹åŒ–å·¥å…·åˆ—è¡¨
        
        # åˆ›å»º v2 AI æœåŠ¡å™¨
        ai_server = AiServer(
            openai_api_key=openai_api_key,
            mcp_tool_execute=mcp_tool_execute,
            default_model="gpt-4",
            default_temperature=0.7
        )
        
        logger.info("âœ… v2 AI æœåŠ¡å™¨å®ä¾‹åˆ›å»ºæˆåŠŸ")
    
    return ai_server


def get_ai_server_with_mcp_configs_v2(api_key: Optional[str] = None, base_url: Optional[str] = None, user_id: Optional[str] = None) -> AiServer:
    """è·å–å¸¦æœ‰ MCP é…ç½®çš„ v2 AI æœåŠ¡å™¨å®ä¾‹"""
    # åŠ è½½ MCP é…ç½®
    http_servers, stdio_servers = load_mcp_configs_sync(user_id=user_id)
    
    # è·å– API å¯†é’¥ - ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„å‚æ•°ï¼Œå…¶æ¬¡ä½¿ç”¨ç¯å¢ƒå˜é‡
    if not api_key:
        api_key = os.getenv("OPENAI_API_KEY")
    
    # å¦‚æœä»ç„¶æ²¡æœ‰APIå¯†é’¥ï¼Œä½¿ç”¨é»˜è®¤å€¼ï¼ˆæŸäº›æ¨¡å‹å¯èƒ½ä¸éœ€è¦ï¼‰
    if not api_key:
        logger.warning("æœªæä¾›APIå¯†é’¥ï¼Œå°†ä½¿ç”¨ç©ºå­—ç¬¦ä¸²ä½œä¸ºé»˜è®¤å€¼")
        api_key = ""
    
    # åˆ›å»ºå¸¦é…ç½®çš„ MCP æ‰§è¡Œå™¨
    # å°† HTTP é…ç½®è½¬æ¢ä¸º mcp_servers æ ¼å¼
    mcp_servers = []
    for name, config in http_servers.items():
        mcp_servers.append({
            "name": name,
            "url": config["url"]
        })
    
    # å°† stdio é…ç½®è½¬æ¢ä¸º stdio_mcp_servers æ ¼å¼
    stdio_mcp_servers = []
    for name, config in stdio_servers.items():
        stdio_mcp_servers.append({
            "name": name,
            "command": config["command"],
            "alias": config["alias"],
            "args": config.get("args", []),
            "env": config.get("env", {})
        })
    
    # è®¾ç½®é…ç½®ç›®å½•
    config_dir = os.path.expanduser("~/.mcp_framework/configs")
    
    mcp_tool_execute = McpToolExecute(
        mcp_servers=mcp_servers,
        stdio_mcp_servers=stdio_mcp_servers,
        config_dir=config_dir
    )
    mcp_tool_execute.init()  # åˆå§‹åŒ–å·¥å…·åˆ—è¡¨
    
    # åˆ›å»º v2 AI æœåŠ¡å™¨
    server = AiServer(
        openai_api_key=api_key,
        mcp_tool_execute=mcp_tool_execute,
        default_model="gpt-4",
        default_temperature=0.7,
        base_url=base_url
    )
    
    logger.info("âœ… v2 å¸¦MCPé…ç½®çš„AIæœåŠ¡å™¨å®ä¾‹åˆ›å»ºæˆåŠŸ")
    return server


# ===== æµå¼å“åº”å¤„ç† =====

def create_stream_response_v2(
    session_id: str,
    content: str,
    model_config: Optional[Dict[str, Any]] = None,
    user_id: Optional[str] = None
) -> Generator[str, None, None]:
    """
    åˆ›å»º v2 ç‰ˆæœ¬çš„æµå¼å“åº”
    
    Args:
        session_id: ä¼šè¯ID
        content: æ¶ˆæ¯å†…å®¹
        model_config: æ¨¡å‹é…ç½®
        
    Yields:
        SSEæ ¼å¼çš„æ•°æ®
    """
    try:
        # ä»æ¨¡å‹é…ç½®ä¸­æå–APIå¯†é’¥å’ŒåŸºç¡€URL
        api_key = None
        base_url = None
        if model_config:
            if "api_key" in model_config:
                api_key = model_config["api_key"]
            if "base_url" in model_config:
                base_url = model_config["base_url"]
        
        # è·å–AIæœåŠ¡å™¨å®ä¾‹
        server = get_ai_server_with_mcp_configs_v2(api_key=api_key, base_url=base_url, user_id=user_id)
        
        # è®¾ç½®ä¸ºæŒ‡å®šä¼šè¯çš„AIæœåŠ¡å™¨å®ä¾‹
        set_session_ai_server(session_id, server)
        
        # åˆ›å»ºçº¿ç¨‹å®‰å…¨çš„äº‹ä»¶é˜Ÿåˆ—
        event_queue = queue.Queue(maxsize=1000)  # è®¾ç½®é˜Ÿåˆ—å¤§å°é™åˆ¶
        
        # åˆ›å»ºçº¿ç¨‹é”ç¡®ä¿çº¿ç¨‹å®‰å…¨
        queue_lock = threading.Lock()
        
        # å®šä¹‰å›è°ƒå‡½æ•°
        def on_chunk(chunk: str):
            try:
                with queue_lock:
                    if not event_queue.full():
                        event_queue.put(("chunk", chunk), block=False)
                        logger.debug(f"Added chunk to queue: {chunk[:50]}...")
                    else:
                        logger.warning("Event queue is full, dropping chunk")
            except Exception as e:
                logger.error(f"Error in chunk callback: {e}")
        
        def on_tools_start(tool_calls: List[Dict[str, Any]]):
            try:
                with queue_lock:
                    if not event_queue.full():
                        event_queue.put(("tools_start", {"tool_calls": tool_calls}), block=False)
                        logger.debug(f"Added tools start to queue: {len(tool_calls)} tools")
                    else:
                        logger.warning("Event queue is full, dropping tools start")
            except Exception as e:
                logger.error(f"Error in tools_start callback: {e}")
        
        def on_tools_stream(result: Dict[str, Any]):
            try:
                with queue_lock:
                    if not event_queue.full():
                        event_queue.put(("tools_stream", result), block=False)
                        logger.debug(f"Added tools stream to queue: {result}")
                    else:
                        logger.warning("Event queue is full, dropping tools stream")
            except Exception as e:
                logger.error(f"Error in tools_stream callback: {e}")
        
        def on_tools_end(tool_results: List[Dict[str, Any]]):
            try:
                with queue_lock:
                    if not event_queue.full():
                        event_queue.put(("tools_end", {"tool_results": tool_results}), block=False)
                        logger.debug(f"Added tools end to queue: {len(tool_results)} results")
                    else:
                        logger.warning("Event queue is full, dropping tools end")
            except Exception as e:
                logger.error(f"Error in tools_end callback: {e}")
        
        # åˆ›å»ºä¸€ä¸ªæ ‡å¿—æ¥æ§åˆ¶AIå¤„ç†çº¿ç¨‹
        ai_completed = threading.Event()
        ai_error = None
        ai_result = None
        
        # å¯åŠ¨AIå¤„ç†çº¿ç¨‹
        def ai_worker():
            nonlocal ai_error, ai_result
            try:
                logger.info(f"Starting AI worker for session {session_id}")
                
                # è§£ææ¨¡å‹é…ç½®
                model = model_config.get("model_name", "gpt-4") if model_config else "gpt-4"
                temperature = model_config.get("temperature", 0.7) if model_config else 0.7
                max_tokens = model_config.get("max_tokens", 4000) if model_config else 4000
                use_tools = model_config.get("use_tools", True) if model_config else True
                
                logger.info(f"AI worker config: model={model}, temp={temperature}, tokens={max_tokens}, tools={use_tools}")
                
                # è°ƒç”¨ v2 ç‰ˆæœ¬çš„ chat æ–¹æ³•
                ai_result = server.chat(
                    session_id=session_id,
                    user_message=content,
                    model=model,
                    temperature=temperature,
                    max_tokens=max_tokens,
                    use_tools=use_tools,
                    on_chunk=on_chunk,
                    on_tools_start=on_tools_start,
                    on_tools_stream=on_tools_stream,
                    on_tools_end=on_tools_end
                )
                
                logger.info(f"AI worker completed successfully for session {session_id}")
                
            except Exception as e:
                ai_error = e
                logger.error(f"Error in AI worker: {e}", exc_info=True)
            finally:
                # ç¡®ä¿å®Œæˆæ ‡å¿—è¢«è®¾ç½®
                ai_completed.set()
                # æ·»åŠ å®Œæˆä¿¡å·åˆ°é˜Ÿåˆ—
                try:
                    with queue_lock:
                        event_queue.put(("ai_completed", None), block=False)
                except:
                    pass
        
        # å¯åŠ¨AIå¤„ç†çº¿ç¨‹
        ai_thread = threading.Thread(target=ai_worker, daemon=True)
        ai_thread.start()
        
        # å‘é€å¼€å§‹äº‹ä»¶
        start_event = {
            "type": "start",
            "session_id": session_id,
            "timestamp": datetime.now().isoformat()
        }
        yield f"data: {json.dumps(start_event, ensure_ascii=False)}\n\n"
        
        # å¤„ç†äº‹ä»¶æµ
        completed = False
        last_heartbeat = time.time()
        heartbeat_interval = 30  # 30ç§’å¿ƒè·³é—´éš”
        
        while not completed:
            try:
                # æ£€æŸ¥æ˜¯å¦æœ‰æ–°äº‹ä»¶
                try:
                    event_type, data = event_queue.get(timeout=2.0)  # å¢åŠ è¶…æ—¶æ—¶é—´
                    
                    if event_type == "chunk":
                        event_data = {
                            "type": "chunk",
                            "content": data,
                            "timestamp": datetime.now().isoformat()
                        }
                        yield f"data: {json.dumps(event_data, ensure_ascii=False)}\n\n"
                        
                    elif event_type == "tools_start":
                        event_data = {
                            "type": "tools_start",
                            "data": data,
                            "timestamp": datetime.now().isoformat()
                        }
                        yield f"data: {json.dumps(event_data, ensure_ascii=False)}\n\n"
                        
                    elif event_type == "tools_stream":
                        event_data = {
                            "type": "tools_stream",
                            "data": data,
                            "timestamp": datetime.now().isoformat()
                        }
                        yield f"data: {json.dumps(event_data, ensure_ascii=False)}\n\n"
                        
                    elif event_type == "tools_end":
                        event_data = {
                            "type": "tools_end",
                            "data": data,
                            "timestamp": datetime.now().isoformat()
                        }
                        yield f"data: {json.dumps(event_data, ensure_ascii=False)}\n\n"
                        
                    elif event_type == "ai_completed":
                        # AIå¤„ç†å®Œæˆä¿¡å·
                        completed = True
                        
                except queue.Empty:
                    # æ£€æŸ¥AIçº¿ç¨‹æ˜¯å¦å®Œæˆ
                    if ai_completed.is_set():
                        completed = True
                    else:
                        # å‘é€å¿ƒè·³
                        current_time = time.time()
                        if current_time - last_heartbeat > heartbeat_interval:
                            heartbeat_event = {
                                "type": "heartbeat",
                                "timestamp": datetime.now().isoformat()
                            }
                            yield f"data: {json.dumps(heartbeat_event, ensure_ascii=False)}\n\n"
                            last_heartbeat = current_time
                
            except Exception as e:
                logger.error(f"Error in stream processing: {e}", exc_info=True)
                error_event = {
                    "type": "error",
                    "error": str(e),
                    "timestamp": datetime.now().isoformat()
                }
                yield f"data: {json.dumps(error_event, ensure_ascii=False)}\n\n"
                break
        
        # ç­‰å¾…AIçº¿ç¨‹å®Œæˆï¼ˆæœ€å¤šç­‰å¾…5ç§’ï¼‰
        if ai_thread.is_alive():
            ai_thread.join(timeout=5.0)
        
        # å‘é€æœ€ç»ˆç»“æœ
        if ai_error:
            final_event = {
                "type": "error",
                "error": str(ai_error),
                "timestamp": datetime.now().isoformat()
            }
        else:
            final_event = {
                "type": "complete",
                "result": ai_result,
                "timestamp": datetime.now().isoformat()
            }
        
        yield f"data: {json.dumps(final_event, ensure_ascii=False)}\n\n"
        
        # å‘é€ç»“æŸæ ‡è®°
        yield f"data: [DONE]\n\n"
        
        logger.info(f"Stream response completed for session {session_id}")
        
    except Exception as e:
        logger.error(f"Error in create_stream_response_v2: {e}", exc_info=True)
        error_event = {
            "type": "error",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }
        yield f"data: {json.dumps(error_event, ensure_ascii=False)}\n\n"
        yield f"data: [DONE]\n\n"


# ===== API ç«¯ç‚¹ =====

@router.post("/chat/stream")
def chat_stream_v2(request: ChatRequestV2):
    """v2 ç‰ˆæœ¬çš„æµå¼èŠå¤©ç«¯ç‚¹"""
    try:
        logger.info(f"ğŸ“¨ v2 æ”¶åˆ°æµå¼èŠå¤©è¯·æ±‚: session_id={request.session_id}")
        
        # è½¬æ¢æ¨¡å‹é…ç½®
        model_config = None
        if request.ai_model_config:
            model_config = {
                "model_name": request.ai_model_config.model_name,
                "temperature": request.ai_model_config.temperature,
                "max_tokens": request.ai_model_config.max_tokens,
                "use_tools": request.ai_model_config.use_tools,
                "api_key": request.ai_model_config.api_key,
                "base_url": request.ai_model_config.base_url
            }
        
        # åˆ›å»ºæµå¼å“åº”
        return StreamingResponse(
            create_stream_response_v2(
                session_id=request.session_id,
                content=request.content,
                model_config=model_config,
                user_id=request.user_id
            ),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "Access-Control-Allow-Origin": "*",
                "Access-Control-Allow-Headers": "*",
                "Access-Control-Allow-Methods": "*"
            }
        )
        
    except Exception as e:
        logger.error(f"âŒ v2 æµå¼èŠå¤©å¤„ç†å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/chat")
def chat_v2(request: ChatRequestV2):
    """v2 ç‰ˆæœ¬çš„æ™®é€šèŠå¤©ç«¯ç‚¹"""
    try:
        logger.info(f"ğŸ“¨ v2 æ”¶åˆ°èŠå¤©è¯·æ±‚: session_id={request.session_id}")
        
        # è·å–AIæœåŠ¡å™¨å®ä¾‹
        server = get_ai_server_with_mcp_configs_v2()
        
        # æå–æ¨¡å‹é…ç½®
        model = request.ai_model_config.model_name if request.ai_model_config else "gpt-4"
        temperature = request.ai_model_config.temperature if request.ai_model_config else 0.7
        max_tokens = request.ai_model_config.max_tokens if request.ai_model_config else 4000
        use_tools = request.ai_model_config.use_tools if request.ai_model_config else True
        
        # è°ƒç”¨ v2 ç‰ˆæœ¬çš„ chat æ–¹æ³•
        result = server.chat(
            session_id=request.session_id,
            user_message=request.content,
            model=model,
            temperature=temperature,
            max_tokens=max_tokens,
            use_tools=use_tools
        )
        
        logger.info(f"âœ… v2 èŠå¤©å¤„ç†å®Œæˆ: session_id={request.session_id}")
        return result
        
    except Exception as e:
        logger.error(f"âŒ v2 èŠå¤©å¤„ç†å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/tools")
def get_available_tools_v2():
    """v2 ç‰ˆæœ¬çš„è·å–å¯ç”¨å·¥å…·ç«¯ç‚¹"""
    try:
        server = get_ai_server_with_mcp_configs_v2()
        tools = server.get_available_tools()
        
        return {
            "success": True,
            "tools": tools,
            "count": len(tools)
        }
        
    except Exception as e:
        logger.error(f"âŒ v2 è·å–å·¥å…·åˆ—è¡¨å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/status")
def get_server_status_v2():
    """v2 ç‰ˆæœ¬çš„è·å–æœåŠ¡å™¨çŠ¶æ€ç«¯ç‚¹"""
    try:
        server = get_ai_server_with_mcp_configs_v2()
        status = server.get_server_status()
        
        # æ·»åŠ  v2 ç‰¹æœ‰ä¿¡æ¯
        status["version"] = "v2"
        status["active_sessions"] = get_active_sessions()
        status["session_count"] = len(get_active_sessions())
        
        return status
        
    except Exception as e:
        logger.error(f"âŒ v2 è·å–æœåŠ¡å™¨çŠ¶æ€å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/session/{session_id}/reset")
def reset_session_v2(session_id: str):
    """v2 ç‰ˆæœ¬çš„é‡ç½®ä¼šè¯ç«¯ç‚¹"""
    try:
        server = get_ai_server_with_mcp_configs_v2()
        result = server.reset_session(session_id)
        
        # ç§»é™¤ä¼šè¯çº§åˆ«çš„æœåŠ¡å™¨å®ä¾‹
        remove_session_ai_server(session_id)
        
        logger.info(f"âœ… v2 ä¼šè¯é‡ç½®å®Œæˆ: session_id={session_id}")
        return result
        
    except Exception as e:
        logger.error(f"âŒ v2 ä¼šè¯é‡ç½®å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/session/{session_id}/config")
def get_session_config_v2(session_id: str):
    """v2 ç‰ˆæœ¬çš„è·å–ä¼šè¯é…ç½®ç«¯ç‚¹"""
    try:
        server = get_session_ai_server(session_id)
        if not server:
            server = get_ai_server_with_mcp_configs_v2()
        
        # è·å–ä¼šè¯é…ç½®
        config = {
            "model": server.get_session_config(session_id, "model", server.default_model),
            "temperature": server.get_session_config(session_id, "temperature", server.default_temperature),
            "session_id": session_id
        }
        
        return {
            "success": True,
            "config": config
        }
        
    except Exception as e:
        logger.error(f"âŒ v2 è·å–ä¼šè¯é…ç½®å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/session/{session_id}/config")
def update_session_config_v2(session_id: str, config: Dict[str, Any]):
    """v2 ç‰ˆæœ¬çš„æ›´æ–°ä¼šè¯é…ç½®ç«¯ç‚¹"""
    try:
        server = get_session_ai_server(session_id)
        if not server:
            server = get_ai_server_with_mcp_configs_v2()
            set_session_ai_server(session_id, server)
        
        # æ›´æ–°ä¼šè¯é…ç½®
        server.update_session_config(session_id, config)
        
        logger.info(f"âœ… v2 ä¼šè¯é…ç½®æ›´æ–°å®Œæˆ: session_id={session_id}")
        return {
            "success": True,
            "message": "ä¼šè¯é…ç½®æ›´æ–°æˆåŠŸ"
        }
        
    except Exception as e:
        logger.error(f"âŒ v2 ä¼šè¯é…ç½®æ›´æ–°å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))