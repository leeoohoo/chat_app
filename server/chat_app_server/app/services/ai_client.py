"""
AIå®¢æˆ·ç«¯ - Pythonå®ç°
å¯¹åº”TypeScriptä¸­çš„AiClientç±»
"""
import json
import logging
from typing import Dict, Any, List, Optional, Callable
from datetime import datetime
from dataclasses import dataclass
from enum import Enum

from .ai_request_handler import AiRequestHandler, AiModelConfig, Message, ToolCall, CallbackType
from .tool_result_processor import ToolResultProcessor
from .message_manager import MessageManager
from .mcp_tool_execute import McpToolExecute

logger = logging.getLogger(__name__)



class AiClient:
    """AIå®¢æˆ·ç«¯ç±»ï¼Œå¤„ç†æ¶ˆæ¯æµå’Œå·¥å…·è°ƒç”¨"""
    
    def __init__(
        self,
        messages: List[Message],
        conversation_id: str,
        tools: List[Any],
        model_config: AiModelConfig,
        callback: Callable[[CallbackType, Any], None],
        mcp_tool_execute: Optional[McpToolExecute] = None,
        message_manager: Optional[MessageManager] = None,
        config_url: str = "/api",
        external_abort_controller: Optional[Any] = None,
        session_id: Optional[str] = None
    ):
        self.messages = messages
        self.conversation_id = conversation_id
        self.tools = tools
        self.model_config = model_config
        self.callback = callback
        self.mcp_tool_execute = mcp_tool_execute
        self.message_manager = message_manager
        self.config_url = config_url
        self.session_id = session_id or conversation_id
        
        # çŠ¶æ€ç®¡ç†
        self.is_aborted = False
        self.current_ai_request_handler: Optional[AiRequestHandler] = None
        
        # åˆå§‹åŒ–ç»„ä»¶
        if message_manager:
            self.tool_result_processor = ToolResultProcessor(self)
        else:
            self.tool_result_processor = None
        
        logger.info(f"AiClient initialized - configUrl: {self.config_url}")

    def start(self) -> None:
        """å¼€å§‹AIå¯¹è¯å¤„ç†"""
        try:
            max_rounds = 10
            current_round = 0
            
            while current_round < max_rounds:
                if self.is_aborted:
                    logger.info("AI processing aborted by user")
                    break
                    
                current_round += 1
                logger.info(f"Starting round {current_round}")
                
                # å¤„ç†AIå“åº”
                has_tool_calls = self._process_ai_response()
                
                if self.is_aborted:
                    break
                
                # å¦‚æœæœ‰å·¥å…·è°ƒç”¨ï¼Œæ‰§è¡Œå·¥å…·
                if has_tool_calls:
                    last_message = self.messages[-1] if self.messages else None
                    if (last_message and 
                        last_message.role == "assistant" and 
                        last_message.tool_calls and 
                        len(last_message.tool_calls) > 0):
                        
                        # æ‰§è¡Œå·¥å…·è°ƒç”¨
                        self._execute_tools(last_message.tool_calls)
                        
                        if self.is_aborted:
                            break
                        
                        # ç»§ç»­ä¸‹ä¸€è½®å¤„ç†
                        continue
                else:
                    # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œå¯¹è¯ç»“æŸ
                    # ai_request_handlerå·²ç»åœ¨æ²¡æœ‰å·¥å…·è°ƒç”¨æ—¶å‘é€äº†ON_COMPLETEäº‹ä»¶ï¼Œè¿™é‡Œä¸éœ€è¦é‡å¤å‘é€
                    logger.info("ğŸ¯ Conversation completed, ON_COMPLETE event already sent by ai_request_handler")
                    break
                    
            if current_round >= max_rounds:
                self.callback(CallbackType.ON_ERROR, "Maximum rounds reached")
                
        except Exception as e:
            import traceback
            error_details = f"Error in AI processing: {str(e)}\nTraceback: {traceback.format_exc()}"
            logger.error(error_details)
            self.callback(CallbackType.ON_ERROR, str(e))

    def start_sync(self) -> None:
        """å¼€å§‹AIå¯¹è¯å¤„ç†ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼Œä¸startæ–¹æ³•ç›¸åŒï¼‰"""
        return self.start()

    def _execute_tools(self, tool_calls: List[ToolCall]) -> None:
        """æ‰§è¡Œå·¥å…·è°ƒç”¨"""
        if not self.mcp_tool_execute:
            logger.warning("No MCP tool executor available")
            return
            
        try:
            logger.info(f"ğŸ”§ [TOOL_EXECUTION] Starting execution of {len(tool_calls)} tool calls")
            
            # å°†ToolCallå¯¹è±¡è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„å­—å…¸
            serializable_tool_calls = []
            for tool_call in tool_calls:
                if hasattr(tool_call, 'to_dict'):
                    serializable_tool_calls.append(tool_call.to_dict())
                else:
                    # å¦‚æœæ˜¯å­—å…¸æ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨
                    serializable_tool_calls.append(tool_call)
            
            # æ‰“å°å·¥å…·è°ƒç”¨è¯¦æƒ…
            for i, tool_call_dict in enumerate(serializable_tool_calls):
                tool_name = tool_call_dict.get("function", {}).get("name") or tool_call_dict.get("name")
                tool_args = tool_call_dict.get("function", {}).get("arguments", {})
                tool_id = tool_call_dict.get("id", "unknown")
                logger.info(f"ğŸ”§ [TOOL_CALL_{i+1}] Tool: {tool_name}, ID: {tool_id}")
                logger.info(f"ğŸ”§ [TOOL_ARGS_{i+1}] Arguments: {json.dumps(tool_args, ensure_ascii=False, indent=2)}")
            
            # æ³¨æ„ï¼šON_TOOL_CALLäº‹ä»¶å·²ç»åœ¨ai_request_handler.pyä¸­å‘é€ï¼Œè¿™é‡Œä¸éœ€è¦é‡å¤å‘é€
            
            # å­˜å‚¨å·¥å…·å“åº”æ¶ˆæ¯ï¼Œç”¨äºæ·»åŠ åˆ°å¯¹è¯å†å²
            tool_response_messages = []
            
            # æ‰§è¡Œæ¯ä¸ªå·¥å…·è°ƒç”¨
            for i, tool_call in enumerate(tool_calls):
                if self.is_aborted:
                    break
                    
                # ä»ToolCallå¯¹è±¡è·å–å·¥å…·åç§°
                if hasattr(tool_call, 'function') and tool_call.function:
                    tool_name = tool_call.function.get("name")
                else:
                    tool_name = None
                    
                if not tool_name:
                    logger.warning(f"ğŸ”§ [TOOL_SKIP_{i+1}] Skipping tool call without name")
                    continue
                    
                # æ£€æŸ¥å·¥å…·æ˜¯å¦æ”¯æŒæµå¼è¾“å‡º
                supports_streaming = self.mcp_tool_execute.supports_streaming(tool_name)
                
                if supports_streaming:
                    logger.info(f"ğŸ”§ [TOOL_EXEC_{i+1}] Using streaming execution for tool: {tool_name}")
                    
                    # æå–å·¥å…·å‚æ•°
                    tool_arguments = {}
                    if hasattr(tool_call, 'function') and tool_call.function:
                        tool_arguments = tool_call.function.get("arguments", {})
                        if isinstance(tool_arguments, str):
                            try:
                                tool_arguments = json.loads(tool_arguments)
                            except json.JSONDecodeError:
                                tool_arguments = {}
                    
                    tool_call_id = tool_call.id if hasattr(tool_call, 'id') else f"call_{tool_name}_{int(datetime.now().timestamp())}"
                    
                    logger.info(f"ğŸ”§ [STREAM_START_{i+1}] Tool: {tool_name}, ID: {tool_call_id}")
                    logger.info(f"ğŸ”§ [STREAM_ARGS_{i+1}] Arguments: {json.dumps(tool_arguments, ensure_ascii=False, indent=2)}")
                    
                    # æµå¼æ‰§è¡Œå·¥å…·
                    tool_result = {"content": "", "tool_name": tool_name}
                    chunk_count = 0
                    
                    def on_chunk(chunk: str):
                        nonlocal chunk_count
                        chunk_count += 1
                        processed_chunk = self._process_chunk(chunk)
                        tool_result["content"] += processed_chunk
                        
                        logger.info(f"ğŸ”§ [STREAM_CHUNK_{i+1}_{chunk_count}] Tool: {tool_name}, Chunk: {processed_chunk[:100]}{'...' if len(processed_chunk) > 100 else ''}")
                        
                        # é€šçŸ¥å‰ç«¯å·¥å…·æµå¼æ•°æ®
                        self.callback(CallbackType.ON_TOOL_STREAM_CHUNK, {
                            "tool_call_id": tool_call_id,
                            "chunk": processed_chunk,
                            "tool_name": tool_name
                        })
                    
                    def on_complete():
                        logger.info(f"ğŸ”§ [STREAM_COMPLETE_{i+1}] Tool: {tool_name}, Total chunks: {chunk_count}, Total length: {len(tool_result['content'])}")
                    
                    def on_error(error: Exception):
                        logger.error(f"ğŸ”§ [STREAM_ERROR_{i+1}] Tool: {tool_name}, Error: {error}")
                        self.callback(CallbackType.ON_ERROR, str(error))
                    
                    self.mcp_tool_execute.execute_stream_sync(
                        tool_name, tool_arguments, tool_call_id, on_chunk, on_complete, on_error
                    )
                    
                    # å¤„ç†å·¥å…·ç»“æœ
                    logger.info(f"ğŸ”§ [STREAM_RAW_RESULT_{i+1}] Tool: {tool_name}, Raw result length: {len(tool_result['content'])}")
                    logger.info(f"ğŸ”§ [STREAM_RAW_CONTENT_{i+1}] Tool: {tool_name}, Raw content: {tool_result['content'][:500]}{'...' if len(tool_result['content']) > 500 else ''}")
                    
                    if self.tool_result_processor:
                        processed_result = self.tool_result_processor.process_tool_result_sync(
                            tool_call_id, tool_name, tool_result["content"]
                        )
                        logger.info(f"ğŸ”§ [STREAM_PROCESSED_{i+1}] Tool: {tool_name}, Processed result length: {len(processed_result)}")
                    else:
                        processed_result = tool_result["content"]
                        logger.info(f"ğŸ”§ [STREAM_NO_PROCESSOR_{i+1}] Tool: {tool_name}, Using raw result")
                    
                    logger.info(f"ğŸ”§ [STREAM_FINAL_RESULT_{i+1}] Tool: {tool_name}, Final result: {processed_result[:500]}{'...' if len(processed_result) > 500 else ''}")
                    
                    # ä¿å­˜å·¥å…·æ¶ˆæ¯åˆ°æ•°æ®åº“
                    if self.message_manager:
                        try:
                            tool_message_data = {
                                "session_id": self.session_id,
                                "role": "tool",
                                "content": processed_result,
                                "status": "completed",
                                "metadata": {
                                    "tool_call_id": tool_call_id,
                                    "tool_name": tool_name
                                }
                            }
                            
                            saved_tool_message = self.message_manager.save_tool_message_sync(tool_message_data)
                            logger.info(f"ğŸ”§ [STREAM_SAVE_{i+1}] Saved tool message: {tool_name} (ID: {saved_tool_message.id})")
                            
                        except Exception as e:
                            logger.error(f"ğŸ”§ [STREAM_SAVE_ERROR_{i+1}] Failed to save tool message: {e}")
                    
                    # æ„é€ å·¥å…·å“åº”æ¶ˆæ¯
                    tool_response_message = Message(
                        id=f"tool_msg_{tool_call_id}",
                        session_id=self.session_id,
                        role="tool",
                        content=processed_result,
                        status="completed",
                        metadata={"tool_call_id": tool_call_id, "tool_name": tool_name}
                    )
                    tool_response_messages.append(tool_response_message)
                    
                    # æ„é€ æ‰§è¡Œç»“æœç”¨äºå‰ç«¯å›è°ƒ
                    execute_result = {
                        "tool_call_id": tool_call_id,
                        "role": "tool",
                        "name": tool_name,
                        "result": processed_result
                    }
                    
                    logger.info(f"ğŸ”§ [STREAM_CALLBACK_{i+1}] Tool: {tool_name}, Sending result to frontend")
                    # é€šçŸ¥å‰ç«¯å·¥å…·æ‰§è¡Œç»“æœ - åŒ…è£…æˆæ•°ç»„æ ¼å¼ä»¥åŒ¹é…å‰ç«¯æœŸæœ›
                    self.callback(CallbackType.ON_TOOL_RESULT, [execute_result])
                        
                else:
                    # éæµå¼æ‰§è¡Œ
                    logger.info(f"ğŸ”§ [TOOL_EXEC_{i+1}] Using non-streaming execution for tool: {tool_name}")
                    
                    # æå–å·¥å…·å‚æ•°
                    tool_arguments = {}
                    if hasattr(tool_call, 'function') and tool_call.function:
                        tool_arguments = tool_call.function.get("arguments", {})
                        if isinstance(tool_arguments, str):
                            try:
                                tool_arguments = json.loads(tool_arguments)
                            except json.JSONDecodeError:
                                tool_arguments = {}
                    
                    tool_call_id = tool_call.id if hasattr(tool_call, 'id') else f"call_{tool_name}_{int(datetime.now().timestamp())}"
                    
                    logger.info(f"ğŸ”§ [NON_STREAM_START_{i+1}] Tool: {tool_name}, ID: {tool_call_id}")
                    logger.info(f"ğŸ”§ [NON_STREAM_ARGS_{i+1}] Arguments: {json.dumps(tool_arguments, ensure_ascii=False, indent=2)}")
                    
                    # è°ƒç”¨å·¥å…·æ‰§è¡Œ
                    result_content = self.mcp_tool_execute.execute_sync(tool_name, tool_arguments)
                    
                    logger.info(f"ğŸ”§ [NON_STREAM_RAW_RESULT_{i+1}] Tool: {tool_name}, Raw result length: {len(str(result_content))}")
                    logger.info(f"ğŸ”§ [NON_STREAM_RAW_CONTENT_{i+1}] Tool: {tool_name}, Raw content: {str(result_content)[:500]}{'...' if len(str(result_content)) > 500 else ''}")
                    
                    # å¤„ç†å·¥å…·ç»“æœ
                    if self.tool_result_processor:
                        processed_result = self.tool_result_processor.process_tool_result_sync(
                            tool_call_id, tool_name, result_content
                        )
                        logger.info(f"ğŸ”§ [NON_STREAM_PROCESSED_{i+1}] Tool: {tool_name}, Processed result length: {len(processed_result)}")
                    else:
                        processed_result = result_content
                        logger.info(f"ğŸ”§ [NON_STREAM_NO_PROCESSOR_{i+1}] Tool: {tool_name}, Using raw result")
                    
                    logger.info(f"ğŸ”§ [NON_STREAM_FINAL_RESULT_{i+1}] Tool: {tool_name}, Final result: {str(processed_result)[:500]}{'...' if len(str(processed_result)) > 500 else ''}")
                    
                    # ä¿å­˜å·¥å…·æ¶ˆæ¯åˆ°æ•°æ®åº“
                    if self.message_manager:
                        try:
                            tool_message_data = {
                                "session_id": self.session_id,
                                "role": "tool",
                                "content": processed_result,
                                "status": "completed",
                                "metadata": {
                                    "tool_call_id": tool_call_id,
                                    "tool_name": tool_name
                                }
                            }
                            
                            saved_tool_message = self.message_manager.save_tool_message_sync(tool_message_data)
                            logger.info(f"ğŸ”§ [NON_STREAM_SAVE_{i+1}] Saved tool message: {tool_name} (ID: {saved_tool_message.id})")
                            
                        except Exception as e:
                            logger.error(f"ğŸ”§ [NON_STREAM_SAVE_ERROR_{i+1}] Failed to save tool message: {e}")
                    
                    # æ„é€ å·¥å…·å“åº”æ¶ˆæ¯
                    tool_response_message = Message(
                        id=f"tool_msg_{tool_call_id}",
                        session_id=self.session_id,
                        role="tool",
                        content=processed_result,
                        status="completed",
                        metadata={"tool_call_id": tool_call_id, "tool_name": tool_name}
                    )
                    tool_response_messages.append(tool_response_message)
                    
                    # æ„é€ æ‰§è¡Œç»“æœç”¨äºå‰ç«¯å›è°ƒ
                    execute_result = {
                        "tool_call_id": tool_call_id,
                        "role": "tool",
                        "name": tool_name,
                        "result": processed_result
                    }
                    
                    logger.info(f"ğŸ”§ [NON_STREAM_CALLBACK_{i+1}] Tool: {tool_name}, Sending result to frontend")
                
                # é€šçŸ¥å‰ç«¯å·¥å…·æ‰§è¡Œç»“æœ - åŒ…è£…æˆæ•°ç»„æ ¼å¼ä»¥åŒ¹é…å‰ç«¯æœŸæœ›
                self.callback(CallbackType.ON_TOOL_RESULT, [execute_result])
            
            # å°†å·¥å…·å“åº”æ¶ˆæ¯æ·»åŠ åˆ°å¯¹è¯å†å²
            if tool_response_messages:
                logger.info(f"ğŸ”§ [TOOL_COMPLETION] Adding {len(tool_response_messages)} tool response messages to conversation")
                self.messages.extend(tool_response_messages)
            
            logger.info(f"ğŸ”§ [TOOL_EXECUTION] Completed execution of {len(tool_calls)} tool calls successfully")
                
        except Exception as e:
            logger.error(f"ğŸ”§ [TOOL_ERROR] Error executing tools: {e}")
            import traceback
            logger.error(f"ğŸ”§ [TOOL_ERROR_TRACE] Traceback: {traceback.format_exc()}")
            self.callback(CallbackType.ON_ERROR, str(e))

    def _process_ai_response(self) -> bool:
        """å¤„ç†AIå“åº”ï¼Œè¿”å›æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨"""
        try:
            logger.info(f"ğŸ”§ [DEBUG] Starting AI response processing with {len(self.tools)} tools available")
            
            # åˆ›å»ºAIè¯·æ±‚å¤„ç†å™¨
            self.current_ai_request_handler = AiRequestHandler(
                self.messages,
                self.tools,
                self.conversation_id,
                self.callback,
                self.model_config,
                self.config_url,
                self.session_id,
                self.message_manager
            )
            
            # å‘é€èŠå¤©å®Œæˆè¯·æ±‚
            updated_messages = self.current_ai_request_handler.chat_completion()
            
            # æ›´æ–°æ¶ˆæ¯åˆ—è¡¨
            if updated_messages:
                self.messages = updated_messages
                
                # æ£€æŸ¥æœ€åä¸€æ¡æ¶ˆæ¯
                last_message = self.messages[-1] if self.messages else None
                if last_message:
                    logger.info(f"ğŸ”§ [DEBUG] Last message content: {last_message.content[:100]}...")
                    logger.info(f"ğŸ”§ [DEBUG] Last message has tool_calls: {bool(last_message.tool_calls)}")
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨éœ€è¦æ‰§è¡Œ
                    if last_message.tool_calls and len(last_message.tool_calls) > 0:
                        logger.info(f"ğŸ”§ [DEBUG] Found {len(last_message.tool_calls)} tool calls to execute")
                        return True  # æœ‰å·¥å…·è°ƒç”¨
                    else:
                        logger.info("ğŸ”§ [DEBUG] No tool calls found, conversation completed")
                        # ai_request_handleråœ¨æ²¡æœ‰å·¥å…·è°ƒç”¨æ—¶ä¼šè°ƒç”¨ON_COMPLETEï¼Œè¿™é‡Œä¸éœ€è¦é‡å¤è°ƒç”¨
                        return False  # æ²¡æœ‰å·¥å…·è°ƒç”¨
            
            return False  # æ²¡æœ‰æ›´æ–°çš„æ¶ˆæ¯
                    
        except Exception as e:
            import traceback
            error_details = f"Error in AI response processing: {str(e)}\nTraceback: {traceback.format_exc()}"
            logger.error(error_details)
            self.callback(CallbackType.ON_ERROR, str(e))
            return False

    def _process_chunk(self, chunk: Any, has_data: bool = False) -> str:
        """
        å¤„ç†å•ä¸ªchunkæ•°æ®ï¼Œæå–å®é™…æ–‡æœ¬å†…å®¹
        å¯¹åº”TypeScriptä¸­çš„processChunkæ–¹æ³•
        """
        if not chunk or not isinstance(chunk, str):
            return str(chunk) if chunk else ""
        
        try:
            # æ¸…ç†chunkæ•°æ®
            clean_chunk = chunk
            if clean_chunk.startswith('data: '):
                clean_chunk = clean_chunk[6:]
            
            # å°è¯•è§£æJSON
            try:
                parsed_chunk = json.loads(clean_chunk)
                if parsed_chunk and isinstance(parsed_chunk, dict):
                    # æå–å†…å®¹
                    if "content" in parsed_chunk:
                        content = parsed_chunk["content"]
                        if isinstance(content, str) and content.startswith('data: '):
                            return self._process_chunk(content, False)
                        return ("\n" + content) if has_data else content
                    elif "data" in parsed_chunk:
                        data = parsed_chunk["data"]
                        if isinstance(data, str) and data.startswith('data: '):
                            return self._process_chunk(data, True)
                        return data
                    elif "ai_stream_chunk" in parsed_chunk:
                        ai_chunk = parsed_chunk["ai_stream_chunk"]
                        if isinstance(ai_chunk, str) and ai_chunk.startswith('data: '):
                            return self._process_chunk(ai_chunk, False)
                        return ai_chunk
                        
            except json.JSONDecodeError:
                pass
                
            return chunk
            
        except Exception as e:
            logger.error(f"Error processing chunk: {e}")
            return chunk

    def abort(self) -> None:
        """ä¸­æ­¢å½“å‰å¤„ç†"""
        logger.info("ğŸ›‘ [DEBUG] AiClient.abort è¢«è°ƒç”¨")
        self.is_aborted = True
        if self.current_ai_request_handler:
            logger.info(f"ğŸ›‘ [DEBUG] current_ai_request_handlerå­˜åœ¨ï¼Œç±»å‹: {type(self.current_ai_request_handler)}")
            self.current_ai_request_handler.abort()
        else:
            logger.warning("ğŸ›‘ [DEBUG] current_ai_request_handlerä¸ºNoneï¼Œæ— æ³•ä¸­æ­¢è¯·æ±‚")

    def is_request_aborted(self) -> bool:
        """æ£€æŸ¥è¯·æ±‚æ˜¯å¦è¢«ä¸­æ­¢"""
        return self.is_aborted

    def reset_abort_state(self) -> None:
        """é‡ç½®ä¸­æ­¢çŠ¶æ€"""
        self.is_aborted = False