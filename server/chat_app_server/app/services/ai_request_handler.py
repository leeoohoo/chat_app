"""
AIè¯·æ±‚å¤„ç†å™¨ - Pythonå®ç°
å¯¹åº”TypeScriptä¸­çš„AiRequestHandlerç±»
"""
import asyncio
import json
import logging
import time
from typing import Dict, Any, List, Optional, Callable, AsyncGenerator, Union
from datetime import datetime
from dataclasses import dataclass
from enum import Enum
from openai import OpenAI
from openai.types.chat import ChatCompletion, ChatCompletionChunk
from openai.types.chat.chat_completion_chunk import ChoiceDelta
from openai.types.chat.chat_completion_message_tool_call import ChatCompletionMessageToolCall

logger = logging.getLogger(__name__)


class CallbackType(Enum):
    """å›è°ƒç±»å‹æšä¸¾"""
    ON_CHUNK = "onChunk"
    ON_TOOL_CALL = "onToolCall"
    ON_TOOL_RESULT = "onToolResult"
    ON_TOOL_STREAM_CHUNK = "onToolStreamChunk"
    ON_COMPLETE = "onComplete"
    ON_ERROR = "onError"


@dataclass
class AiModelConfig:
    """AIæ¨¡å‹é…ç½®"""
    model_name: str = "gpt-3.5-turbo"
    temperature: float = 0.7
    max_tokens: int = 1000
    api_key: str = ""
    base_url: str = "https://api.openai.com/v1"


@dataclass
class ToolCall:
    """å·¥å…·è°ƒç”¨"""
    id: str
    type: str = "function"
    function: Optional[Dict[str, Any]] = None
    result: Optional[Any] = None
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼ä»¥æ”¯æŒJSONåºåˆ—åŒ–"""
        return {
            "id": self.id,
            "type": self.type,
            "function": self.function,
            "result": self.result
        }


@dataclass
class Message:
    """æ¶ˆæ¯"""
    id: Optional[str] = None
    session_id: Optional[str] = None
    role: str = "user"
    content: str = ""
    status: str = "completed"
    created_at: Optional[datetime] = None
    metadata: Optional[Dict[str, Any]] = None
    tool_calls: Optional[List[ToolCall]] = None
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼ï¼Œç”¨äºJSONåºåˆ—åŒ–"""
        return {
            "id": self.id,
            "session_id": self.session_id,
            "role": self.role,
            "content": self.content,
            "status": self.status,
            "created_at": self.created_at.isoformat() if self.created_at else None,
            "metadata": self.metadata,
            "tool_calls": [
                {
                    "id": tc.id,
                    "type": tc.type,
                    "function": tc.function,
                    "result": tc.result
                }
                for tc in self.tool_calls
            ] if self.tool_calls else None
        }


class AiRequestHandler:
    """AIè¯·æ±‚å¤„ç†å™¨ï¼Œè´Ÿè´£ä¸OpenAI APIäº¤äº’"""
    
    def __init__(
        self,
        messages: List[Message],
        tools: List[Any],
        conversation_id: str,
        callback: Callable[[CallbackType, Any], None],
        model_config: AiModelConfig,
        config_url: str = "/api",
        session_id: Optional[str] = None,
        message_manager = None
    ):
        self.messages = messages
        self.tools = tools
        self.conversation_id = conversation_id
        self.callback = callback
        self.model_config = model_config
        self.config_url = config_url
        self.session_id = session_id or conversation_id
        self.message_manager = message_manager
        self.is_aborted = False
        self.current_task = None  # å­˜å‚¨å½“å‰è¿è¡Œçš„ä»»åŠ¡
        
        # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
        self.client = OpenAI(
            api_key=model_config.api_key,
            base_url=model_config.base_url
        )
        
        logger.info(f"AiRequestHandler initialized - configUrl: {self.config_url}")
        logger.info(f"Model config base_url: {self.model_config.base_url}")

    def chat_completion(self) -> List[Message]:
        """
        å‘é€èŠå¤©å®Œæˆè¯·æ±‚
        è¿”å›æ›´æ–°åçš„æ¶ˆæ¯åˆ—è¡¨
        """
        try:
            if self.is_aborted:
                logger.info("Request aborted before starting")
                return self.messages
            
            # å‡†å¤‡æ¶ˆæ¯æ ¼å¼
            formatted_messages = self._format_messages_for_api()
            
            # å‡†å¤‡è¯·æ±‚å‚æ•°
            request_params = {
                "model": self.model_config.model_name,
                "messages": formatted_messages,
                "temperature": self.model_config.temperature,
                "max_tokens": self.model_config.max_tokens,
                "stream": True
            }
            
            # å¦‚æœæœ‰å·¥å…·ï¼Œæ·»åŠ å·¥å…·å‚æ•°
            if self.tools:
                request_params["tools"] = self.tools
                request_params["tool_choice"] = "auto"
                logger.info(f"ğŸ”§ [DEBUG] å‘é€ç»™AIçš„å·¥å…·æ•°é‡: {len(self.tools)}")
                logger.info(f"ğŸ”§ [DEBUG] å·¥å…·è¯¦æƒ…: {json.dumps(self.tools, indent=2, ensure_ascii=False)}")
            else:
                logger.info("ğŸ”§ [DEBUG] æ²¡æœ‰å·¥å…·å‘é€ç»™AI")
            
            logger.info(f"Sending chat completion request with {len(formatted_messages)} messages")
            logger.info(f"ğŸ”§ [DEBUG] å®Œæ•´è¯·æ±‚å‚æ•°: {json.dumps({k: v for k, v in request_params.items() if k != 'messages'}, indent=2, ensure_ascii=False)}")
            
            # å‘é€åŒæ­¥è¯·æ±‚
            stream = self.client.chat.completions.create(**request_params)
            # å¤„ç†æµå¼å“åº”
            return self._handle_openai_stream_response(stream)
            
            return updated_messages
            
        except Exception as e:
            import traceback
            error_details = f"Chat completion error: {str(e)}\nTraceback: {traceback.format_exc()}"
            logger.error(error_details)
            if self.callback:
                self.callback(CallbackType.ON_ERROR, str(e))
            return self.messages

    def _format_messages_for_api(self) -> List[Dict[str, Any]]:
        """å°†æ¶ˆæ¯æ ¼å¼åŒ–ä¸ºOpenAI APIæ ¼å¼"""
        formatted = []
        
        logger.info(f"ğŸ”§ [FORMAT_DEBUG] Formatting {len(self.messages)} messages for API")
        
        for i, msg in enumerate(self.messages):
            logger.info(f"ğŸ”§ [FORMAT_MSG_{i}] Role: {msg.role}, Content length: {len(msg.content)}, Has tool_calls: {bool(msg.tool_calls)}")
            
            formatted_msg = {
                "role": msg.role,
                "content": msg.content
            }
            
            # å¦‚æœæœ‰å·¥å…·è°ƒç”¨ï¼Œæ·»åŠ tool_callså­—æ®µ
            if msg.tool_calls:
                formatted_msg["tool_calls"] = [tc.to_dict() for tc in msg.tool_calls]
                logger.info(f"ğŸ”§ [FORMAT_TOOL_CALLS_{i}] Added {len(msg.tool_calls)} tool calls")
            
            # å¦‚æœæ˜¯å·¥å…·æ¶ˆæ¯ï¼Œæ·»åŠ tool_call_idå’Œnameå­—æ®µ
            if msg.role == "tool" and msg.metadata:
                tool_call_id = msg.metadata.get("tool_call_id")
                tool_name = msg.metadata.get("tool_name")
                if tool_call_id:
                    formatted_msg["tool_call_id"] = tool_call_id
                    logger.info(f"ğŸ”§ [FORMAT_TOOL_MSG_{i}] Tool call ID: {tool_call_id}, Tool name: {tool_name}")
                if tool_name:
                    formatted_msg["name"] = tool_name
                
            formatted.append(formatted_msg)
            
        logger.info(f"ğŸ”§ [FORMAT_RESULT] Formatted {len(formatted)} messages for API")
        return formatted

    def _handle_openai_stream_response(self, stream) -> List[Message]:
        """
        å¤„ç†OpenAIæµå¼å“åº”
        å¯¹åº”TypeScriptä¸­çš„handleOpenAIStreamResponseæ–¹æ³•
        """
        try:
            accumulated_content = ""
            accumulated_tool_calls = []
            chunk_count = 0
            
            for chunk in stream:
                if self.is_aborted:
                    logger.info("Stream processing aborted")
                    break
                    
                chunk_count += 1
                
                if chunk.choices and len(chunk.choices) > 0:
                    choice = chunk.choices[0]
                    delta = choice.delta
                    
                    # å¤„ç†å†…å®¹å¢é‡
                    if hasattr(delta, 'content') and delta.content:
                        accumulated_content += delta.content
                        
                        if self.callback:
                            self.callback(CallbackType.ON_CHUNK, {
                                "type": "chunk",
                                "content": delta.content,
                                "accumulated": accumulated_content
                            })
                    
                    # å¤„ç†å·¥å…·è°ƒç”¨å¢é‡
                    if hasattr(delta, 'tool_calls') and delta.tool_calls:
                        logger.info(f"ğŸ”§ [DEBUG] Received tool_calls delta with {len(delta.tool_calls)} calls")
                        tool_call_event_sent = False
                        
                        for tool_call_delta in delta.tool_calls:
                            logger.info(f"ğŸ”§ [DEBUG] Processing tool call delta: index={tool_call_delta.index}, id={getattr(tool_call_delta, 'id', None)}")
                            
                            # ç¡®ä¿accumulated_tool_callsæœ‰è¶³å¤Ÿçš„å…ƒç´ 
                            while len(accumulated_tool_calls) <= tool_call_delta.index:
                                accumulated_tool_calls.append({
                                    "id": "",
                                    "type": "function",
                                    "function": {"name": "", "arguments": ""}
                                })
                            
                            # æ›´æ–°å·¥å…·è°ƒç”¨ä¿¡æ¯
                            if tool_call_delta.id:
                                accumulated_tool_calls[tool_call_delta.index]["id"] = tool_call_delta.id
                                logger.info(f"ğŸ”§ [DEBUG] Set tool call id: {tool_call_delta.id}")
                            
                            if tool_call_delta.function:
                                if tool_call_delta.function.name:
                                    accumulated_tool_calls[tool_call_delta.index]["function"]["name"] = tool_call_delta.function.name
                                    logger.info(f"ğŸ”§ [DEBUG] Set tool function name: {tool_call_delta.function.name}")
                                    
                                    # å½“æˆ‘ä»¬ç¬¬ä¸€æ¬¡æ”¶åˆ°å·¥å…·åç§°æ—¶ï¼Œå‘é€ON_TOOL_CALLäº‹ä»¶
                                    if not tool_call_event_sent and self.callback:
                                        # å‡†å¤‡å½“å‰çš„å·¥å…·è°ƒç”¨æ•°æ®ï¼ˆä¸éœ€è¦ç»“æœï¼‰
                                        current_tool_calls = []
                                        for tc in accumulated_tool_calls:
                                            if tc.get("id") and tc.get("function", {}).get("name"):
                                                tool_call_data = {
                                                    "id": tc.get("id", ""),
                                                    "type": tc.get("type", "function"),
                                                    "function": tc.get("function", {})
                                                }
                                                current_tool_calls.append(tool_call_data)
                                        
                                        if current_tool_calls:
                                            logger.info(f"ğŸ”§ [DEBUG] Sending ON_TOOL_CALL event with {len(current_tool_calls)} tool calls")
                                            self.callback(CallbackType.ON_TOOL_CALL, current_tool_calls)
                                            tool_call_event_sent = True
                                
                                if tool_call_delta.function.arguments:
                                    accumulated_tool_calls[tool_call_delta.index]["function"]["arguments"] += tool_call_delta.function.arguments
                                    logger.info(f"ğŸ”§ [DEBUG] Added function arguments: {tool_call_delta.function.arguments}")
                        
                        logger.info(f"ğŸ”§ [DEBUG] Current accumulated_tool_calls count: {len(accumulated_tool_calls)}")
                    
                    # æ£€æŸ¥æ˜¯å¦å®Œæˆ
                    if choice.finish_reason:
                        logger.info(f"Stream finished with reason: {choice.finish_reason}")
                        break
            
            logger.info(f"OpenAI stream completed. Processed {chunk_count} chunks.")
            
            # åˆ›å»ºæ–°çš„åŠ©æ‰‹æ¶ˆæ¯
            assistant_message = Message(
                id=f"msg_{self.session_id}_{len(self.messages)}",
                session_id=self.session_id,
                role="assistant",
                content=accumulated_content,
                status="completed"
            )
            
            # å¦‚æœæœ‰å·¥å…·è°ƒç”¨ï¼Œè½¬æ¢ä¸º ToolCall å¯¹è±¡å¹¶æ·»åŠ åˆ°æ¶ˆæ¯ä¸­
            if accumulated_tool_calls:
                logger.info(f"ğŸ”§ [DEBUG] Found {len(accumulated_tool_calls)} accumulated tool calls")
                for i, tc in enumerate(accumulated_tool_calls):
                    logger.info(f"ğŸ”§ [DEBUG] Tool call {i}: id={tc.get('id')}, function={tc.get('function', {}).get('name')}")
                
                assistant_message.tool_calls = [
                    ToolCall(
                        id=tc.get("id", ""),
                        type=tc.get("type", "function"),
                        function=tc.get("function"),
                        result=tc.get("result")
                    )
                    for tc in accumulated_tool_calls
                ]
                logger.info(f"ğŸ”§ [DEBUG] Created {len(assistant_message.tool_calls)} ToolCall objects")
            else:
                logger.info("ğŸ”§ [DEBUG] No accumulated tool calls found")
            
            # æ›´æ–°æ¶ˆæ¯åˆ—è¡¨
            updated_messages = self.messages.copy()
            updated_messages.append(assistant_message)
            
            # ç›´æ¥ä¿å­˜AIæ¶ˆæ¯åˆ°æ•°æ®åº“
            if self.message_manager and self.session_id:
                try:
                    # å‡†å¤‡å·¥å…·è°ƒç”¨æ•°æ®
                    tool_calls_data = []
                    if accumulated_tool_calls:
                        for tc in accumulated_tool_calls:
                            tool_call_data = {
                                "id": tc.get("id", ""),
                                "type": tc.get("type", "function"),
                                "function": tc.get("function", {})
                            }
                            # åªæœ‰åœ¨æœ‰ç»“æœæ—¶æ‰æ·»åŠ resultå­—æ®µ
                            if isinstance(tc, dict) and tc.get("result"):
                                tool_call_data["result"] = tc.get("result")
                            elif hasattr(tc, 'result') and tc.result:
                                tool_call_data["result"] = tc.result
                            
                            tool_calls_data.append(tool_call_data)
                    
                    # ä¿å­˜åŠ©æ‰‹æ¶ˆæ¯
                    assistant_message_data = {
                        "sessionId": self.session_id,
                        "role": "assistant",
                        "content": assistant_message.content,
                        "status": "completed",
                        "createdAt": datetime.now(),
                        "metadata": {
                            "toolCalls": tool_calls_data
                        },
                        "toolCalls": tool_calls_data
                    }
                    
                    logger.info(f"ğŸ”§ [DEBUG] Directly saving assistant message with {len(tool_calls_data)} tool calls")
                    saved_message = self.message_manager.save_assistant_message_sync(assistant_message_data)
                    logger.info(f"ğŸ¯ Assistant message saved successfully: {saved_message.id}")
                    
                    # ON_TOOL_CALLäº‹ä»¶å·²ç»åœ¨æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨æ—¶å‘é€äº†ï¼Œè¿™é‡Œä¸éœ€è¦é‡å¤å‘é€
                    
                    # å½“æœ‰å·¥å…·è°ƒç”¨æ—¶ï¼Œä¸è§¦å‘å®Œæˆå›è°ƒï¼Œå› ä¸ºå¯¹è¯è¿˜æ²¡æœ‰çœŸæ­£å®Œæˆ
                    # åªæœ‰åœ¨æ²¡æœ‰å·¥å…·è°ƒç”¨çš„æƒ…å†µä¸‹æ‰è§¦å‘å®Œæˆå›è°ƒ
                    if not accumulated_tool_calls and self.callback:
                        self.callback(CallbackType.ON_COMPLETE, {
                            "message": assistant_message,
                            "accumulated_content": accumulated_content,
                            "tool_calls": accumulated_tool_calls,
                            "saved_message": saved_message,
                            "final": True  # æ ‡è®°è¿™æ˜¯æœ€ç»ˆçš„å®Œæˆäº‹ä»¶
                        })
                        
                except Exception as e:
                    logger.error(f"Error saving assistant message: {e}")
                    import traceback
                    logger.error(f"Traceback: {traceback.format_exc()}")
            else:
                logger.warning("âš ï¸ No message_manager or session_id available for saving assistant message")
                # åªæœ‰åœ¨æ²¡æœ‰å·¥å…·è°ƒç”¨çš„æƒ…å†µä¸‹æ‰è§¦å‘å®Œæˆå›è°ƒ
                if not accumulated_tool_calls and self.callback:
                    self.callback(CallbackType.ON_COMPLETE, {
                        "message": assistant_message,
                        "accumulated_content": accumulated_content,
                        "tool_calls": accumulated_tool_calls,
                        "final": True  # æ ‡è®°è¿™æ˜¯æœ€ç»ˆçš„å®Œæˆäº‹ä»¶
                    })
            
            return updated_messages
            
        except Exception as e:
            import traceback
            error_details = f"Error handling OpenAI stream: {str(e)}\nTraceback: {traceback.format_exc()}"
            logger.error(error_details)
            if self.callback:
                self.callback(CallbackType.ON_ERROR, str(e))
            return self.messages

    def abort(self) -> None:
        """ä¸­æ­¢å½“å‰è¯·æ±‚"""
        logger.info("ğŸ›‘ [DEBUG] AiRequestHandler.abort è¢«è°ƒç”¨")
        self.is_aborted = True
        logger.info("ğŸ›‘ [DEBUG] AI request handler aborted")
        
        # å¦‚æœæœ‰æ­£åœ¨è¿è¡Œçš„ä»»åŠ¡ï¼Œå–æ¶ˆå®ƒ
        if self.current_task and not self.current_task.done():
            logger.info(f"ğŸ›‘ [DEBUG] æ­£åœ¨å–æ¶ˆä»»åŠ¡ï¼Œä»»åŠ¡çŠ¶æ€: {self.current_task}")
            self.current_task.cancel()
            logger.info("ğŸ›‘ [DEBUG] Current AI task cancelled")
        else:
            if self.current_task is None:
                logger.warning("ğŸ›‘ [DEBUG] current_taskä¸ºNoneï¼Œæ— æ³•å–æ¶ˆ")
            elif self.current_task.done():
                logger.warning("ğŸ›‘ [DEBUG] current_taskå·²å®Œæˆï¼Œæ— éœ€å–æ¶ˆ")

    def is_request_aborted(self) -> bool:
        """æ£€æŸ¥è¯·æ±‚æ˜¯å¦è¢«ä¸­æ­¢"""
        return self.is_aborted